{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFTpUVPDYrJ7"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "editdistance\n",
        "lmdb\n",
        "matplotlib\n",
        "numpy\n",
        "tensorflow\n",
        "Pillow\n",
        "streamlit\n",
        "pyngrok\n",
        "reportlab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8ekobqoYaVW"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHD20s3vp3tQ"
      },
      "outputs": [],
      "source": [
        "!mkdir src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDXaygcus58-"
      },
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTcjyoobtAQy"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data/characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjl0zDVwiYNc"
      },
      "outputs": [],
      "source": [
        "%%writefile src/character_recognition.py\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "from typing import Tuple, List\n",
        "\n",
        "import cv2\n",
        "import editdistance\n",
        "from pathlib import Path\n",
        "\n",
        "from dataloader_iam import DataLoaderIAM, Batch\n",
        "from model import Model, DecoderType\n",
        "from preprocessor import Preprocessor\n",
        "\n",
        "\n",
        "class FilePaths:\n",
        "    \"\"\"Filenames and paths to data.\"\"\"\n",
        "    fn_char_list = '../model/charList.txt'\n",
        "    fn_summary = '../model/summary.json'\n",
        "    fn_corpus = '../data/corpus.txt'\n",
        "\n",
        "\n",
        "def get_img_height() -> int:\n",
        "    \"\"\"Fixed height for NN.\"\"\"\n",
        "    return 32\n",
        "\n",
        "\n",
        "def get_img_size(line_mode: bool = False) -> Tuple[int, int]:\n",
        "    \"\"\"Height is fixed for NN, width is set according to training mode (single words or text lines).\"\"\"\n",
        "    if line_mode:\n",
        "        return 256, get_img_height()\n",
        "    return 128, get_img_height()\n",
        "\n",
        "\n",
        "def write_summary(char_error_rates: List[float], word_accuracies: List[float]) -> None:\n",
        "    \"\"\"Writes training summary file for NN.\"\"\"\n",
        "    with open(FilePaths.fn_summary, 'w') as f:\n",
        "        json.dump({'charErrorRates': char_error_rates, 'wordAccuracies': word_accuracies}, f)\n",
        "\n",
        "\n",
        "def train(model: Model,\n",
        "          loader: DataLoaderIAM,\n",
        "          line_mode: bool,\n",
        "          early_stopping: int = 25) -> None:\n",
        "    \"\"\"Trains NN.\"\"\"\n",
        "    epoch = 0  # number of training epochs since start\n",
        "    summary_char_error_rates = []\n",
        "    summary_word_accuracies = []\n",
        "    preprocessor = Preprocessor(get_img_size(line_mode), data_augmentation=True, line_mode=line_mode)\n",
        "    best_char_error_rate = float('inf')  # best validation character error rate\n",
        "    no_improvement_since = 0  # number of epochs no improvement of character error rate occurred\n",
        "    # stop training after this number of epochs without improvement\n",
        "    while True:\n",
        "        epoch += 1\n",
        "        print('Epoch:', epoch)\n",
        "\n",
        "        # train\n",
        "        print('Train NN')\n",
        "        loader.train_set()\n",
        "        while loader.has_next():\n",
        "            iter_info = loader.get_iterator_info()\n",
        "            batch = loader.get_next()\n",
        "            batch = preprocessor.process_batch(batch)\n",
        "            loss = model.train_batch(batch)\n",
        "            print(f'Epoch: {epoch} Batch: {iter_info[0]}/{iter_info[1]} Loss: {loss}')\n",
        "\n",
        "        # validate\n",
        "        char_error_rate, word_accuracy = validate(model, loader, line_mode)\n",
        "\n",
        "        # write summary\n",
        "        summary_char_error_rates.append(char_error_rate)\n",
        "        summary_word_accuracies.append(word_accuracy)\n",
        "        write_summary(summary_char_error_rates, summary_word_accuracies)\n",
        "\n",
        "        # if best validation accuracy so far, save model parameters\n",
        "        if char_error_rate < best_char_error_rate:\n",
        "            print('Character error rate improved, save model')\n",
        "            best_char_error_rate = char_error_rate\n",
        "            no_improvement_since = 0\n",
        "            model.save()\n",
        "        else:\n",
        "            print(f'Character error rate not improved, best so far: {char_error_rate * 100.0}%')\n",
        "            no_improvement_since += 1\n",
        "\n",
        "        # stop training if no more improvement in the last x epochs\n",
        "        if no_improvement_since >= early_stopping:\n",
        "            print(f'No more improvement since {early_stopping} epochs. Training stopped.')\n",
        "            break\n",
        "\n",
        "\n",
        "def validate(model: Model, loader: DataLoaderIAM, line_mode: bool) -> Tuple[float, float]:\n",
        "    \"\"\"Validates NN.\"\"\"\n",
        "    print('Validate NN')\n",
        "    loader.validation_set()\n",
        "    preprocessor = Preprocessor(get_img_size(line_mode), line_mode=line_mode)\n",
        "    num_char_err = 0\n",
        "    num_char_total = 0\n",
        "    num_word_ok = 0\n",
        "    num_word_total = 0\n",
        "    while loader.has_next():\n",
        "        iter_info = loader.get_iterator_info()\n",
        "        print(f'Batch: {iter_info[0]} / {iter_info[1]}')\n",
        "        batch = loader.get_next()\n",
        "        batch = preprocessor.process_batch(batch)\n",
        "        recognized, _ = model.infer_batch(batch)\n",
        "\n",
        "        print('Ground truth -> Recognized')\n",
        "        for i in range(len(recognized)):\n",
        "            num_word_ok += 1 if batch.gt_texts[i] == recognized[i] else 0\n",
        "            num_word_total += 1\n",
        "            dist = editdistance.eval(recognized[i], batch.gt_texts[i])\n",
        "            num_char_err += dist\n",
        "            num_char_total += len(batch.gt_texts[i])\n",
        "            print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + batch.gt_texts[i] + '\"', '->',\n",
        "                  '\"' + recognized[i] + '\"')\n",
        "\n",
        "    # print validation result\n",
        "    char_error_rate = num_char_err / num_char_total\n",
        "    word_accuracy = num_word_ok / num_word_total\n",
        "    print(f'Character error rate: {char_error_rate * 100.0}%. Word accuracy: {word_accuracy * 100.0}%.')\n",
        "    return char_error_rate, word_accuracy\n",
        "\n",
        "\n",
        "def infer(model: Model, fn_img: Path) -> None:\n",
        "    \"\"\"Recognizes text in image provided by file path.\"\"\"\n",
        "    img = cv2.imread(fn_img, cv2.IMREAD_GRAYSCALE)\n",
        "    assert img is not None\n",
        "\n",
        "    preprocessor = Preprocessor(get_img_size(), dynamic_width=True, padding=16)\n",
        "    img = preprocessor.process_img(img)\n",
        "\n",
        "    batch = Batch([img], None, 1)\n",
        "    recognized, probability = model.infer_batch(batch, True)\n",
        "    print(f'Recognized: \"{recognized[0]}\"')\n",
        "    print(f'Probability: {probability[0]}')\n",
        "    return recognized[0]\n",
        "\n",
        "\n",
        "# Slightly edited to allow for custom file names to be analysed\n",
        "def main():\n",
        "    \"\"\"Main function.\"\"\"\n",
        "    CharacterFile = \"\"\n",
        "    Files = os.listdir(\"/content/data/characters\")\n",
        "    for File in Files:\n",
        "        if \"Cropped\" in File:\n",
        "            CharacterFile = File\n",
        "    print(CharacterFile)\n",
        "    if CharacterFile != \"\":\n",
        "        parser = argparse.ArgumentParser()\n",
        "\n",
        "        parser.add_argument('--mode', choices=['train', 'validate', 'infer'], default='infer')\n",
        "        parser.add_argument('--decoder', choices=['bestpath', 'beamsearch', 'wordbeamsearch'], default='bestpath')\n",
        "        parser.add_argument('--batch_size', help='Batch size.', type=int, default=100)\n",
        "        parser.add_argument('--data_dir', help='Directory containing IAM dataset.', type=Path, required=False)\n",
        "        parser.add_argument('--fast', help='Load samples from LMDB.', action='store_true')\n",
        "        parser.add_argument('--line_mode', help='Train to read text lines instead of single words.',\n",
        "                            action='store_true')\n",
        "        parser.add_argument('--img_file', help='Image used for inference.', type=Path,\n",
        "                            default='../data/characters/{}'.format(CharacterFile))\n",
        "        parser.add_argument('--early_stopping', help='Early stopping epochs.', type=int, default=25)\n",
        "        parser.add_argument('--dump', help='Dump output of NN to CSV file(s).', action='store_true')\n",
        "        args = parser.parse_args()\n",
        "\n",
        "        # set chosen CTC decoder\n",
        "        decoder_mapping = {'bestpath': DecoderType.BestPath,\n",
        "                           'beamsearch': DecoderType.BeamSearch,\n",
        "                           'wordbeamsearch': DecoderType.WordBeamSearch}\n",
        "        decoder_type = decoder_mapping[args.decoder]\n",
        "\n",
        "        # train or validate on IAM dataset\n",
        "        if args.mode in ['train', 'validate']:\n",
        "            # load training data, create TF model\n",
        "            loader = DataLoaderIAM(args.data_dir, args.batch_size, fast=args.fast)\n",
        "            char_list = loader.char_list\n",
        "\n",
        "            # when in line mode, take care to have a whitespace in the char list\n",
        "            if args.line_mode and ' ' not in char_list:\n",
        "                char_list = [' '] + char_list\n",
        "\n",
        "            # save characters of model for inference mode\n",
        "            open(FilePaths.fn_char_list, 'w').write(''.join(char_list))\n",
        "\n",
        "            # save words contained in dataset into file\n",
        "            open(FilePaths.fn_corpus, 'w').write(' '.join(loader.train_words + loader.validation_words))\n",
        "\n",
        "            # execute training or validation\n",
        "            if args.mode == 'train':\n",
        "                model = Model(char_list, decoder_type)\n",
        "                train(model, loader, line_mode=args.line_mode, early_stopping=args.early_stopping)\n",
        "            elif args.mode == 'validate':\n",
        "                model = Model(char_list, decoder_type, must_restore=True)\n",
        "                validate(model, loader, args.line_mode)\n",
        "\n",
        "        # infer text on image\n",
        "        elif args.mode == 'infer':\n",
        "            model = Model(list(open(FilePaths.fn_char_list).read()), decoder_type, must_restore=True, dump=args.dump)\n",
        "            # Saving the new file as the character name\n",
        "            Character = infer(model, args.img_file)\n",
        "            if Character == '\"':\n",
        "                Character = \"SM\"\n",
        "            if Character == \"?\":\n",
        "                Character = \"QM\"\n",
        "            if Character == \"/\":\n",
        "                Character = \"FS\"\n",
        "            if Character == '\\\\':\n",
        "                Character = \"BS\"\n",
        "            if Character == \"*\":\n",
        "                Character = \"AS\"\n",
        "            if Character == \"<\":\n",
        "                Character = \"LT\"\n",
        "            if Character == \">\":\n",
        "                Character = \"GT\"\n",
        "            if Character == \"|\":\n",
        "                Character = \"VL\"\n",
        "\n",
        "            try:\n",
        "                os.rename(\"../data/characters/\" + CharacterFile, \"../data/characters/\" + Character + \".png\")\n",
        "            except IOError:\n",
        "                NewFile = False\n",
        "                Counter = 1\n",
        "                while not NewFile:\n",
        "                    try:\n",
        "                        os.rename(\"../data/characters/\" + CharacterFile,\n",
        "                                  \"../data/characters/{}{}.png\".format(Character, \".\" * Counter))\n",
        "                        NewFile = True\n",
        "                    except IOError:\n",
        "                        Counter = Counter + 1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkAsHCTYio7R"
      },
      "outputs": [],
      "source": [
        "%%writefile src/create_lmdb.py\n",
        "import argparse\n",
        "import pickle\n",
        "\n",
        "import cv2\n",
        "import lmdb\n",
        "from pathlib import Path\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data_dir', type=Path, required=True)\n",
        "args = parser.parse_args()\n",
        "\n",
        "# 2GB is enough for IAM dataset\n",
        "assert not (args.data_dir / 'lmdb').exists()\n",
        "env = lmdb.open(str(args.data_dir / 'lmdb'), map_size=1024 * 1024 * 1024 * 2)\n",
        "\n",
        "# go over all png files\n",
        "fn_imgs = list((args.data_dir / 'img').walkfiles('*.png'))\n",
        "\n",
        "# and put the imgs into lmdb as pickled grayscale imgs\n",
        "with env.begin(write=True) as txn:\n",
        "    for i, fn_img in enumerate(fn_imgs):\n",
        "        print(i, len(fn_imgs))\n",
        "        img = cv2.imread(fn_img, cv2.IMREAD_GRAYSCALE)\n",
        "        basename = fn_img.basename()\n",
        "        txn.put(basename.encode(\"ascii\"), pickle.dumps(img))\n",
        "\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yChz1Dyiv7j"
      },
      "outputs": [],
      "source": [
        "%%writefile src/dataloader_iam.py\n",
        "import pickle\n",
        "import random\n",
        "from collections import namedtuple\n",
        "from typing import Tuple\n",
        "\n",
        "import cv2\n",
        "import lmdb\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "Sample = namedtuple('Sample', 'gt_text, file_path')\n",
        "Batch = namedtuple('Batch', 'imgs, gt_texts, batch_size')\n",
        "\n",
        "\n",
        "class DataLoaderIAM:\n",
        "    \"\"\"\n",
        "    Loads data which corresponds to IAM format,\n",
        "    see: http://www.fki.inf.unibe.ch/databases/iam-handwriting-database\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_dir: Path,\n",
        "                 batch_size: int,\n",
        "                 data_split: float = 0.95,\n",
        "                 fast: bool = True) -> None:\n",
        "        \"\"\"Loader for dataset.\"\"\"\n",
        "\n",
        "        assert data_dir.exists()\n",
        "\n",
        "        self.fast = fast\n",
        "        if fast:\n",
        "            self.env = lmdb.open(str(data_dir / 'lmdb'), readonly=True)\n",
        "\n",
        "        self.data_augmentation = False\n",
        "        self.curr_idx = 0\n",
        "        self.batch_size = batch_size\n",
        "        self.samples = []\n",
        "\n",
        "        f = open(data_dir / 'gt/words.txt')\n",
        "        chars = set()\n",
        "        bad_samples_reference = ['a01-117-05-02', 'r06-022-03-05']  # known broken images in IAM dataset\n",
        "        for line in f:\n",
        "            # ignore comment line\n",
        "            if not line or line[0] == '#':\n",
        "                continue\n",
        "\n",
        "            line_split = line.strip().split(' ')\n",
        "            assert len(line_split) >= 9\n",
        "\n",
        "            # filename: part1-part2-part3 --> part1/part1-part2/part1-part2-part3.png\n",
        "            file_name_split = line_split[0].split('-')\n",
        "            file_name_subdir1 = file_name_split[0]\n",
        "            file_name_subdir2 = f'{file_name_split[0]}-{file_name_split[1]}'\n",
        "            file_base_name = line_split[0] + '.png'\n",
        "            file_name = data_dir / 'img' / file_name_subdir1 / file_name_subdir2 / file_base_name\n",
        "\n",
        "            if line_split[0] in bad_samples_reference:\n",
        "                print('Ignoring known broken image:', file_name)\n",
        "                continue\n",
        "\n",
        "            # GT text are columns starting at 9\n",
        "            gt_text = ' '.join(line_split[8:])\n",
        "            chars = chars.union(set(list(gt_text)))\n",
        "\n",
        "            # put sample into list\n",
        "            self.samples.append(Sample(gt_text, file_name))\n",
        "\n",
        "        # split into training and validation set: 95% - 5%\n",
        "        split_idx = int(data_split * len(self.samples))\n",
        "        self.train_samples = self.samples[:split_idx]\n",
        "        self.validation_samples = self.samples[split_idx:]\n",
        "\n",
        "        # put words into lists\n",
        "        self.train_words = [x.gt_text for x in self.train_samples]\n",
        "        self.validation_words = [x.gt_text for x in self.validation_samples]\n",
        "\n",
        "        # start with train set\n",
        "        self.train_set()\n",
        "\n",
        "        # list of all chars in dataset\n",
        "        self.char_list = sorted(list(chars))\n",
        "\n",
        "    def train_set(self) -> None:\n",
        "        \"\"\"Switch to randomly chosen subset of training set.\"\"\"\n",
        "        self.data_augmentation = True\n",
        "        self.curr_idx = 0\n",
        "        random.shuffle(self.train_samples)\n",
        "        self.samples = self.train_samples\n",
        "        self.curr_set = 'train'\n",
        "\n",
        "    def validation_set(self) -> None:\n",
        "        \"\"\"Switch to validation set.\"\"\"\n",
        "        self.data_augmentation = False\n",
        "        self.curr_idx = 0\n",
        "        self.samples = self.validation_samples\n",
        "        self.curr_set = 'val'\n",
        "\n",
        "    def get_iterator_info(self) -> Tuple[int, int]:\n",
        "        \"\"\"Current batch index and overall number of batches.\"\"\"\n",
        "        if self.curr_set == 'train':\n",
        "            num_batches = int(np.floor(len(self.samples) / self.batch_size))  # train set: only full-sized batches\n",
        "        else:\n",
        "            num_batches = int(np.ceil(len(self.samples) / self.batch_size))  # val set: allow last batch to be smaller\n",
        "        curr_batch = self.curr_idx // self.batch_size + 1\n",
        "        return curr_batch, num_batches\n",
        "\n",
        "    def has_next(self) -> bool:\n",
        "        \"\"\"Is there a next element?\"\"\"\n",
        "        if self.curr_set == 'train':\n",
        "            return self.curr_idx + self.batch_size <= len(self.samples)  # train set: only full-sized batches\n",
        "        else:\n",
        "            return self.curr_idx < len(self.samples)  # val set: allow last batch to be smaller\n",
        "\n",
        "    def _get_img(self, i: int) -> np.ndarray:\n",
        "        if self.fast:\n",
        "            with self.env.begin() as txn:\n",
        "                basename = Path(self.samples[i].file_path).basename()\n",
        "                data = txn.get(basename.encode(\"ascii\"))\n",
        "                img = pickle.loads(data)\n",
        "        else:\n",
        "            img = cv2.imread(self.samples[i].file_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def get_next(self) -> Batch:\n",
        "        \"\"\"Get next element.\"\"\"\n",
        "        batch_range = range(self.curr_idx, min(self.curr_idx + self.batch_size, len(self.samples)))\n",
        "\n",
        "        imgs = [self._get_img(i) for i in batch_range]\n",
        "        gt_texts = [self.samples[i].gt_text for i in batch_range]\n",
        "\n",
        "        self.curr_idx += self.batch_size\n",
        "        return Batch(imgs, gt_texts, len(imgs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phn6Zs6Di2to"
      },
      "outputs": [],
      "source": [
        "%%writefile src/model.py\n",
        "import os\n",
        "import sys\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from dataloader_iam import Batch\n",
        "\n",
        "# Disable eager mode\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "class DecoderType:\n",
        "    \"\"\"CTC decoder types.\"\"\"\n",
        "    BestPath = 0\n",
        "    BeamSearch = 1\n",
        "    WordBeamSearch = 2\n",
        "\n",
        "\n",
        "class Model:\n",
        "    \"\"\"Minimalistic TF model for HTR.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 char_list: List[str],\n",
        "                 decoder_type: str = DecoderType.BestPath,\n",
        "                 must_restore: bool = False,\n",
        "                 dump: bool = False) -> None:\n",
        "        \"\"\"Init model: add CNN, RNN and CTC and initialize TF.\"\"\"\n",
        "        self.dump = dump\n",
        "        self.char_list = char_list\n",
        "        self.decoder_type = decoder_type\n",
        "        self.must_restore = must_restore\n",
        "        self.snap_ID = 0\n",
        "\n",
        "        # Whether to use normalization over a batch or a population\n",
        "        self.is_train = tf.compat.v1.placeholder(tf.bool, name='is_train')\n",
        "\n",
        "        # input image batch\n",
        "        self.input_imgs = tf.compat.v1.placeholder(tf.float32, shape=(None, None, None))\n",
        "\n",
        "        # setup CNN, RNN and CTC\n",
        "        self.setup_cnn()\n",
        "        self.setup_rnn()\n",
        "        self.setup_ctc()\n",
        "\n",
        "        # setup optimizer to train NN\n",
        "        self.batches_trained = 0\n",
        "        self.update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
        "        with tf.control_dependencies(self.update_ops):\n",
        "            self.optimizer = tf.compat.v1.train.AdamOptimizer().minimize(self.loss)\n",
        "\n",
        "        # initialize TF\n",
        "        self.sess, self.saver = self.setup_tf()\n",
        "\n",
        "    def setup_cnn(self) -> None:\n",
        "        \"\"\"Create CNN layers.\"\"\"\n",
        "        cnn_in4d = tf.expand_dims(input=self.input_imgs, axis=3)\n",
        "\n",
        "        # list of parameters for the layers\n",
        "        kernel_vals = [5, 5, 3, 3, 3]\n",
        "        feature_vals = [1, 32, 64, 128, 128, 256]\n",
        "        stride_vals = pool_vals = [(2, 2), (2, 2), (1, 2), (1, 2), (1, 2)]\n",
        "        num_layers = len(stride_vals)\n",
        "\n",
        "        # create layers\n",
        "        pool = cnn_in4d  # input to first CNN layer\n",
        "        for i in range(num_layers):\n",
        "            kernel = tf.Variable(\n",
        "                tf.random.truncated_normal([kernel_vals[i], kernel_vals[i], feature_vals[i], feature_vals[i + 1]],\n",
        "                                           stddev=0.1))\n",
        "            conv = tf.nn.conv2d(input=pool, filters=kernel, padding='SAME', strides=(1, 1, 1, 1))\n",
        "            conv_norm = tf.compat.v1.layers.batch_normalization(conv, training=self.is_train)\n",
        "            relu = tf.nn.relu(conv_norm)\n",
        "            pool = tf.nn.max_pool2d(input=relu, ksize=(1, pool_vals[i][0], pool_vals[i][1], 1),\n",
        "                                    strides=(1, stride_vals[i][0], stride_vals[i][1], 1), padding='VALID')\n",
        "\n",
        "        self.cnn_out_4d = pool\n",
        "\n",
        "    def setup_rnn(self) -> None:\n",
        "        \"\"\"Create RNN layers.\"\"\"\n",
        "        rnn_in3d = tf.squeeze(self.cnn_out_4d, axis=[2])\n",
        "\n",
        "        # basic cells which is used to build RNN\n",
        "        num_hidden = 256\n",
        "        cells = [tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=num_hidden, state_is_tuple=True) for _ in\n",
        "                 range(2)]  # 2 layers\n",
        "\n",
        "        # stack basic cells\n",
        "        stacked = tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)\n",
        "\n",
        "        # bidirectional RNN\n",
        "        # BxTxF -> BxTx2H\n",
        "        (fw, bw), _ = tf.compat.v1.nn.bidirectional_dynamic_rnn(cell_fw=stacked, cell_bw=stacked, inputs=rnn_in3d,\n",
        "                                                                dtype=rnn_in3d.dtype)\n",
        "\n",
        "        # BxTxH + BxTxH -> BxTx2H -> BxTx1X2H\n",
        "        concat = tf.expand_dims(tf.concat([fw, bw], 2), 2)\n",
        "\n",
        "        # project output to chars (including blank): BxTx1x2H -> BxTx1xC -> BxTxC\n",
        "        kernel = tf.Variable(tf.random.truncated_normal([1, 1, num_hidden * 2, len(self.char_list) + 1], stddev=0.1))\n",
        "        self.rnn_out_3d = tf.squeeze(tf.nn.atrous_conv2d(value=concat, filters=kernel, rate=1, padding='SAME'),\n",
        "                                     axis=[2])\n",
        "\n",
        "    def setup_ctc(self) -> None:\n",
        "        \"\"\"Create CTC loss and decoder.\"\"\"\n",
        "        # BxTxC -> TxBxC\n",
        "        self.ctc_in_3d_tbc = tf.transpose(a=self.rnn_out_3d, perm=[1, 0, 2])\n",
        "        # ground truth text as sparse tensor\n",
        "        self.gt_texts = tf.SparseTensor(tf.compat.v1.placeholder(tf.int64, shape=[None, 2]),\n",
        "                                        tf.compat.v1.placeholder(tf.int32, [None]),\n",
        "                                        tf.compat.v1.placeholder(tf.int64, [2]))\n",
        "\n",
        "        # calc loss for batch\n",
        "        self.seq_len = tf.compat.v1.placeholder(tf.int32, [None])\n",
        "        self.loss = tf.reduce_mean(\n",
        "            input_tensor=tf.compat.v1.nn.ctc_loss(labels=self.gt_texts, inputs=self.ctc_in_3d_tbc,\n",
        "                                                  sequence_length=self.seq_len,\n",
        "                                                  ctc_merge_repeated=True))\n",
        "\n",
        "        # calc loss for each element to compute label probability\n",
        "        self.saved_ctc_input = tf.compat.v1.placeholder(tf.float32,\n",
        "                                                        shape=[None, None, len(self.char_list) + 1])\n",
        "        self.loss_per_element = tf.compat.v1.nn.ctc_loss(labels=self.gt_texts, inputs=self.saved_ctc_input,\n",
        "                                                         sequence_length=self.seq_len, ctc_merge_repeated=True)\n",
        "\n",
        "        # best path decoding or beam search decoding\n",
        "        if self.decoder_type == DecoderType.BestPath:\n",
        "            self.decoder = tf.nn.ctc_greedy_decoder(inputs=self.ctc_in_3d_tbc, sequence_length=self.seq_len)\n",
        "        elif self.decoder_type == DecoderType.BeamSearch:\n",
        "            self.decoder = tf.nn.ctc_beam_search_decoder(inputs=self.ctc_in_3d_tbc, sequence_length=self.seq_len,\n",
        "                                                         beam_width=50)\n",
        "        # word beam search decoding (see https://github.com/githubharald/CTCWordBeamSearch)\n",
        "        elif self.decoder_type == DecoderType.WordBeamSearch:\n",
        "            # prepare information about language (dictionary, characters in dataset, characters forming words)\n",
        "            chars = ''.join(self.char_list)\n",
        "            word_chars = open('../model/wordCharList.txt').read().splitlines()[0]\n",
        "            corpus = open('../data/corpus.txt').read()\n",
        "\n",
        "            # decode using the \"Words\" mode of word beam search\n",
        "            from word_beam_search import WordBeamSearch\n",
        "            self.decoder = WordBeamSearch(50, 'Words', 0.0, corpus.encode('utf8'), chars.encode('utf8'),\n",
        "                                          word_chars.encode('utf8'))\n",
        "\n",
        "            # the input to the decoder must have softmax already applied\n",
        "            self.wbs_input = tf.nn.softmax(self.ctc_in_3d_tbc, axis=2)\n",
        "\n",
        "    def setup_tf(self) -> Tuple[tf.compat.v1.Session, tf.compat.v1.train.Saver]:\n",
        "        \"\"\"Initialize TF.\"\"\"\n",
        "        print('Python: ' + sys.version)\n",
        "        print('Tensorflow: ' + tf.__version__)\n",
        "\n",
        "        sess = tf.compat.v1.Session()  # TF session\n",
        "\n",
        "        saver = tf.compat.v1.train.Saver(max_to_keep=1)  # saver saves model to file\n",
        "        model_dir = '../model/'\n",
        "        latest_snapshot = tf.train.latest_checkpoint(model_dir)  # is there a saved model?\n",
        "\n",
        "        # if model must be restored (for inference), there must be a snapshot\n",
        "        if self.must_restore and not latest_snapshot:\n",
        "            raise Exception('No saved model found in: ' + model_dir)\n",
        "\n",
        "        # load saved model if available\n",
        "        if latest_snapshot:\n",
        "            print('Init with stored values from ' + latest_snapshot)\n",
        "            saver.restore(sess, latest_snapshot)\n",
        "        else:\n",
        "            print('Init with new values')\n",
        "            sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "        return sess, saver\n",
        "\n",
        "    def to_sparse(self, texts: List[str]) -> Tuple[List[List[int]], List[int], List[int]]:\n",
        "        \"\"\"Put ground truth texts into sparse tensor for ctc_loss.\"\"\"\n",
        "        indices = []\n",
        "        values = []\n",
        "        shape = [len(texts), 0]  # last entry must be max(labelList[i])\n",
        "\n",
        "        # go over all texts\n",
        "        for batchElement, text in enumerate(texts):\n",
        "            # convert to string of label (i.e. class-ids)\n",
        "            label_str = [self.char_list.index(c) for c in text]\n",
        "            # sparse tensor must have size of max. label-string\n",
        "            if len(label_str) > shape[1]:\n",
        "                shape[1] = len(label_str)\n",
        "            # put each label into sparse tensor\n",
        "            for i, label in enumerate(label_str):\n",
        "                indices.append([batchElement, i])\n",
        "                values.append(label)\n",
        "\n",
        "        return indices, values, shape\n",
        "\n",
        "    def decoder_output_to_text(self, ctc_output: tuple, batch_size: int) -> List[str]:\n",
        "        \"\"\"Extract texts from output of CTC decoder.\"\"\"\n",
        "\n",
        "        # word beam search: already contains label strings\n",
        "        if self.decoder_type == DecoderType.WordBeamSearch:\n",
        "            label_strs = ctc_output\n",
        "\n",
        "        # TF decoders: label strings are contained in sparse tensor\n",
        "        else:\n",
        "            # ctc returns tuple, first element is SparseTensor\n",
        "            decoded = ctc_output[0][0]\n",
        "\n",
        "            # contains string of labels for each batch element\n",
        "            label_strs = [[] for _ in range(batch_size)]\n",
        "\n",
        "            # go over all indices and save mapping: batch -> values\n",
        "            for (idx, idx2d) in enumerate(decoded.indices):\n",
        "                label = decoded.values[idx]\n",
        "                batch_element = idx2d[0]  # index according to [b,t]\n",
        "                label_strs[batch_element].append(label)\n",
        "\n",
        "        # map labels to chars for all batch elements\n",
        "        return [''.join([self.char_list[c] for c in labelStr]) for labelStr in label_strs]\n",
        "\n",
        "    def train_batch(self, batch: Batch) -> float:\n",
        "        \"\"\"Feed a batch into the NN to train it.\"\"\"\n",
        "        num_batch_elements = len(batch.imgs)\n",
        "        max_text_len = batch.imgs[0].shape[0] // 4\n",
        "        sparse = self.to_sparse(batch.gt_texts)\n",
        "        eval_list = [self.optimizer, self.loss]\n",
        "        feed_dict = {self.input_imgs: batch.imgs, self.gt_texts: sparse,\n",
        "                     self.seq_len: [max_text_len] * num_batch_elements, self.is_train: True}\n",
        "        _, loss_val = self.sess.run(eval_list, feed_dict)\n",
        "        self.batches_trained += 1\n",
        "        return loss_val\n",
        "\n",
        "    @staticmethod\n",
        "    def dump_nn_output(rnn_output: np.ndarray) -> None:\n",
        "        \"\"\"Dump the output of the NN to CSV file(s).\"\"\"\n",
        "        dump_dir = '../dump/'\n",
        "        if not os.path.isdir(dump_dir):\n",
        "            os.mkdir(dump_dir)\n",
        "\n",
        "        # iterate over all batch elements and create a CSV file for each one\n",
        "        max_t, max_b, max_c = rnn_output.shape\n",
        "        for b in range(max_b):\n",
        "            csv = ''\n",
        "            for t in range(max_t):\n",
        "                for c in range(max_c):\n",
        "                    csv += str(rnn_output[t, b, c]) + ';'\n",
        "                csv += '\\n'\n",
        "            fn = dump_dir + 'rnnOutput_' + str(b) + '.csv'\n",
        "            print('Write dump of NN to file: ' + fn)\n",
        "            with open(fn, 'w') as f:\n",
        "                f.write(csv)\n",
        "\n",
        "    def infer_batch(self, batch: Batch, calc_probability: bool = False, probability_of_gt: bool = False):\n",
        "        \"\"\"Feed a batch into the NN to recognize the texts.\"\"\"\n",
        "\n",
        "        # decode, optionally save RNN output\n",
        "        num_batch_elements = len(batch.imgs)\n",
        "\n",
        "        # put tensors to be evaluated into list\n",
        "        eval_list = []\n",
        "\n",
        "        if self.decoder_type == DecoderType.WordBeamSearch:\n",
        "            eval_list.append(self.wbs_input)\n",
        "        else:\n",
        "            eval_list.append(self.decoder)\n",
        "\n",
        "        if self.dump or calc_probability:\n",
        "            eval_list.append(self.ctc_in_3d_tbc)\n",
        "\n",
        "        # sequence length depends on input image size (model downsizes width by 4)\n",
        "        max_text_len = batch.imgs[0].shape[0] // 4\n",
        "\n",
        "        # dict containing all tensor fed into the model\n",
        "        feed_dict = {self.input_imgs: batch.imgs, self.seq_len: [max_text_len] * num_batch_elements,\n",
        "                     self.is_train: False}\n",
        "\n",
        "        # evaluate model\n",
        "        eval_res = self.sess.run(eval_list, feed_dict)\n",
        "\n",
        "        # TF decoders: decoding already done in TF graph\n",
        "        if self.decoder_type != DecoderType.WordBeamSearch:\n",
        "            decoded = eval_res[0]\n",
        "        # word beam search decoder: decoding is done in C++ function compute()\n",
        "        else:\n",
        "            decoded = self.decoder.compute(eval_res[0])\n",
        "\n",
        "        # map labels (numbers) to character string\n",
        "        texts = self.decoder_output_to_text(decoded, num_batch_elements)\n",
        "\n",
        "        # feed RNN output and recognized text into CTC loss to compute labeling probability\n",
        "        probs = None\n",
        "        if calc_probability:\n",
        "            sparse = self.to_sparse(batch.gt_texts) if probability_of_gt else self.to_sparse(texts)\n",
        "            ctc_input = eval_res[1]\n",
        "            eval_list = self.loss_per_element\n",
        "            feed_dict = {self.saved_ctc_input: ctc_input, self.gt_texts: sparse,\n",
        "                         self.seq_len: [max_text_len] * num_batch_elements, self.is_train: False}\n",
        "            loss_vals = self.sess.run(eval_list, feed_dict)\n",
        "            probs = np.exp(-loss_vals)\n",
        "\n",
        "        # dump the output of the NN to CSV file(s)\n",
        "        if self.dump:\n",
        "            self.dump_nn_output(eval_res[1])\n",
        "\n",
        "        return texts, probs\n",
        "\n",
        "    def save(self) -> None:\n",
        "        \"\"\"Save model to file.\"\"\"\n",
        "        self.snap_ID += 1\n",
        "        self.saver.save(self.sess, '../model/snapshot', global_step=self.snap_ID)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0El8T3n1i5un"
      },
      "outputs": [],
      "source": [
        "%%writefile src/preprocessor.py\n",
        "import random\n",
        "from typing import Tuple\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "from dataloader_iam import Batch\n",
        "\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self,\n",
        "                 img_size: Tuple[int, int],\n",
        "                 padding: int = 0,\n",
        "                 dynamic_width: bool = False,\n",
        "                 data_augmentation: bool = False,\n",
        "                 line_mode: bool = False) -> None:\n",
        "        # dynamic width only supported when no data augmentation happens\n",
        "        assert not (dynamic_width and data_augmentation)\n",
        "        # when padding is on, we need dynamic width enabled\n",
        "        assert not (padding > 0 and not dynamic_width)\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.padding = padding\n",
        "        self.dynamic_width = dynamic_width\n",
        "        self.data_augmentation = data_augmentation\n",
        "        self.line_mode = line_mode\n",
        "\n",
        "    @staticmethod\n",
        "    def _truncate_label(text: str, max_text_len: int) -> str:\n",
        "        \"\"\"\n",
        "        Function ctc_loss can't compute loss if it cannot find a mapping between text label and input\n",
        "        labels. Repeat letters cost double because of the blank symbol needing to be inserted.\n",
        "        If a too-long label is provided, ctc_loss returns an infinite gradient.\n",
        "        \"\"\"\n",
        "        cost = 0\n",
        "        for i in range(len(text)):\n",
        "            if i != 0 and text[i] == text[i - 1]:\n",
        "                cost += 2\n",
        "            else:\n",
        "                cost += 1\n",
        "            if cost > max_text_len:\n",
        "                return text[:i]\n",
        "        return text\n",
        "\n",
        "    def _simulate_text_line(self, batch: Batch) -> Batch:\n",
        "        \"\"\"Create image of a text line by pasting multiple word images into an image.\"\"\"\n",
        "\n",
        "        default_word_sep = 30\n",
        "        default_num_words = 5\n",
        "\n",
        "        # go over all batch elements\n",
        "        res_imgs = []\n",
        "        res_gt_texts = []\n",
        "        for i in range(batch.batch_size):\n",
        "            # number of words to put into current line\n",
        "            num_words = random.randint(1, 8) if self.data_augmentation else default_num_words\n",
        "\n",
        "            # concat ground truth texts\n",
        "            curr_gt = ' '.join([batch.gt_texts[(i + j) % batch.batch_size] for j in range(num_words)])\n",
        "            res_gt_texts.append(curr_gt)\n",
        "\n",
        "            # put selected word images into list, compute target image size\n",
        "            sel_imgs = []\n",
        "            word_seps = [0]\n",
        "            h = 0\n",
        "            w = 0\n",
        "            for j in range(num_words):\n",
        "                curr_sel_img = batch.imgs[(i + j) % batch.batch_size]\n",
        "                curr_word_sep = random.randint(20, 50) if self.data_augmentation else default_word_sep\n",
        "                h = max(h, curr_sel_img.shape[0])\n",
        "                w += curr_sel_img.shape[1]\n",
        "                sel_imgs.append(curr_sel_img)\n",
        "                if j + 1 < num_words:\n",
        "                    w += curr_word_sep\n",
        "                    word_seps.append(curr_word_sep)\n",
        "\n",
        "            # put all selected word images into target image\n",
        "            target = np.ones([h, w], np.uint8) * 255\n",
        "            x = 0\n",
        "            for curr_sel_img, curr_word_sep in zip(sel_imgs, word_seps):\n",
        "                x += curr_word_sep\n",
        "                y = (h - curr_sel_img.shape[0]) // 2\n",
        "                target[y:y + curr_sel_img.shape[0]:, x:x + curr_sel_img.shape[1]] = curr_sel_img\n",
        "                x += curr_sel_img.shape[1]\n",
        "\n",
        "            # put image of line into result\n",
        "            res_imgs.append(target)\n",
        "\n",
        "        return Batch(res_imgs, res_gt_texts, batch.batch_size)\n",
        "\n",
        "    def process_img(self, img: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Resize to target size, apply data augmentation.\"\"\"\n",
        "\n",
        "        # there are damaged files in IAM dataset - just use black image instead\n",
        "        if img is None:\n",
        "            img = np.zeros(self.img_size[::-1])\n",
        "\n",
        "        # data augmentation\n",
        "        img = img.astype(np.float)\n",
        "        if self.data_augmentation:\n",
        "            # photometric data augmentation\n",
        "            if random.random() < 0.25:\n",
        "                def rand_odd():\n",
        "                    return random.randint(1, 3) * 2 + 1\n",
        "                img = cv2.GaussianBlur(img, (rand_odd(), rand_odd()), 0)\n",
        "            if random.random() < 0.25:\n",
        "                img = cv2.dilate(img, np.ones((3, 3)))\n",
        "            if random.random() < 0.25:\n",
        "                img = cv2.erode(img, np.ones((3, 3)))\n",
        "\n",
        "            # geometric data augmentation\n",
        "            wt, ht = self.img_size\n",
        "            h, w = img.shape\n",
        "            f = min(wt / w, ht / h)\n",
        "            fx = f * np.random.uniform(0.75, 1.05)\n",
        "            fy = f * np.random.uniform(0.75, 1.05)\n",
        "\n",
        "            # random position around center\n",
        "            txc = (wt - w * fx) / 2\n",
        "            tyc = (ht - h * fy) / 2\n",
        "            freedom_x = max((wt - fx * w) / 2, 0)\n",
        "            freedom_y = max((ht - fy * h) / 2, 0)\n",
        "            tx = txc + np.random.uniform(-freedom_x, freedom_x)\n",
        "            ty = tyc + np.random.uniform(-freedom_y, freedom_y)\n",
        "\n",
        "            # map image into target image\n",
        "            M = np.float32([[fx, 0, tx], [0, fy, ty]])\n",
        "            target = np.ones(self.img_size[::-1]) * 255\n",
        "            img = cv2.warpAffine(img, M, dsize=self.img_size, dst=target, borderMode=cv2.BORDER_TRANSPARENT)\n",
        "\n",
        "            # photometric data augmentation\n",
        "            if random.random() < 0.5:\n",
        "                img = img * (0.25 + random.random() * 0.75)\n",
        "            if random.random() < 0.25:\n",
        "                img = np.clip(img + (np.random.random(img.shape) - 0.5) * random.randint(1, 25), 0, 255)\n",
        "            if random.random() < 0.1:\n",
        "                img = 255 - img\n",
        "\n",
        "        # no data augmentation\n",
        "        else:\n",
        "            if self.dynamic_width:\n",
        "                ht = self.img_size[1]\n",
        "                h, w = img.shape\n",
        "                f = ht / h\n",
        "                wt = int(f * w + self.padding)\n",
        "                wt = wt + (4 - wt) % 4\n",
        "                tx = (wt - w * f) / 2\n",
        "                ty = 0\n",
        "            else:\n",
        "                wt, ht = self.img_size\n",
        "                h, w = img.shape\n",
        "                f = min(wt / w, ht / h)\n",
        "                tx = (wt - w * f) / 2\n",
        "                ty = (ht - h * f) / 2\n",
        "\n",
        "            # map image into target image\n",
        "            M = np.float32([[f, 0, tx], [0, f, ty]])\n",
        "            target = np.ones([ht, wt]) * 255\n",
        "            img = cv2.warpAffine(img, M, dsize=(wt, ht), dst=target, borderMode=cv2.BORDER_TRANSPARENT)\n",
        "\n",
        "        # transpose for TF\n",
        "        img = cv2.transpose(img)\n",
        "\n",
        "        # convert to range [-1, 1]\n",
        "        img = img / 255 - 0.5\n",
        "        return img\n",
        "\n",
        "    def process_batch(self, batch: Batch) -> Batch:\n",
        "        if self.line_mode:\n",
        "            batch = self._simulate_text_line(batch)\n",
        "\n",
        "        res_imgs = [self.process_img(img) for img in batch.imgs]\n",
        "        max_text_len = res_imgs[0].shape[0] // 4\n",
        "        res_gt_texts = [self._truncate_label(gt_text, max_text_len) for gt_text in batch.gt_texts]\n",
        "        return Batch(res_imgs, res_gt_texts, batch.batch_size)\n",
        "\n",
        "\n",
        "def main():\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    img = cv2.imread('../data/test.png', cv2.IMREAD_GRAYSCALE)\n",
        "    img_aug = Preprocessor((256, 32), data_augmentation=True).process_img(img)\n",
        "    plt.subplot(121)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.subplot(122)\n",
        "    plt.imshow(cv2.transpose(img_aug) + 0.5, cmap='gray', vmin=0, vmax=1)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nhk9Uldsjgoh"
      },
      "outputs": [],
      "source": [
        "%%writefile content.txt\n",
        "AAA BIG HAIRY DOG SAT ON MY HEAD\n",
        "DOG           IT WAS VERY HEAVY\n",
        "SAT\n",
        "ON                   I EXCLAIMED LOUDLY\n",
        "MY\n",
        "\n",
        "\n",
        "\n",
        "      HELP THERE IS  A DOG ON MY HEAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WArBtpMPjPVd"
      },
      "outputs": [],
      "source": [
        "!python src/character_recognition.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBaRGa4Xjb6N"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import subprocess\n",
        "import threading\n",
        "import tempfile\n",
        "import shutil\n",
        "from PIL import Image, ImageFilter, ImageTk, ImageDraw\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from io import BytesIO\n",
        "import zipfile\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.pagesizes import letter, A4\n",
        "from reportlab.lib.utils import ImageReader\n",
        "import soundfile as sf\n",
        "\n",
        "# OCR integration\n",
        "try:\n",
        "    from transformers import pipeline\n",
        "    OCR_AVAILABLE = True\n",
        "    @st.cache_resource\n",
        "    def load_ocr_model():\n",
        "        \"\"\"Load OCR model with caching\"\"\"\n",
        "        return pipeline('image-to-text', model=\"microsoft/trocr-base-handwritten\")\n",
        "except ImportError:\n",
        "    OCR_AVAILABLE = False\n",
        "    st.warning(\"OCR functionality not available. Install transformers: pip install transformers torch\")\n",
        "\n",
        "# Speech-to-Text integration\n",
        "try:\n",
        "    import torch\n",
        "    from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
        "    STT_AVAILABLE = True\n",
        "\n",
        "    @st.cache_resource\n",
        "    def load_speech_model():\n",
        "        \"\"\"Load Speech-to-Text model with caching\"\"\"\n",
        "        model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
        "        processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
        "        return model, processor\n",
        "except ImportError:\n",
        "    STT_AVAILABLE = False\n",
        "    st.warning(\"Speech-to-Text not available. Install: pip install torch transformers datasets soundfile\")\n",
        "\n",
        "# Configure Streamlit page\n",
        "st.set_page_config(\n",
        "    page_title=\"HandCraft AI\",\n",
        "    page_icon=\"✍️\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# Custom CSS for better styling\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .main-header {\n",
        "        font-size: 3rem;\n",
        "        font-weight: bold;\n",
        "        text-align: center;\n",
        "        color: #ffffff;\n",
        "        margin-bottom: 2rem;\n",
        "        text-shadow: 2px 2px 4px rgba(0,0,0,0.3);\n",
        "    }\n",
        "    .step-header {\n",
        "        font-size: 1.8rem;\n",
        "        font-weight: bold;\n",
        "        color: #64b5f6;\n",
        "        margin-top: 2rem;\n",
        "        margin-bottom: 1rem;\n",
        "        border-bottom: 2px solid #64b5f6;\n",
        "        padding-bottom: 0.5rem;\n",
        "    }\n",
        "    .info-box {\n",
        "        background: linear-gradient(135deg, #1a237e 0%, #3949ab 100%);\n",
        "        color: #ffffff;\n",
        "        padding: 1.5rem;\n",
        "        border-radius: 12px;\n",
        "        border-left: 4px solid #64b5f6;\n",
        "        margin: 1rem 0;\n",
        "        box-shadow: 0 4px 12px rgba(0,0,0,0.3);\n",
        "    }\n",
        "    .success-box {\n",
        "        background: linear-gradient(135deg, #2e7d32 0%, #4caf50 100%);\n",
        "        color: #ffffff;\n",
        "        padding: 1.5rem;\n",
        "        border-radius: 12px;\n",
        "        border-left: 4px solid #81c784;\n",
        "        margin: 1rem 0;\n",
        "        box-shadow: 0 4px 12px rgba(0,0,0,0.3);\n",
        "    }\n",
        "    .warning-box {\n",
        "        background: linear-gradient(135deg, #f57c00 0%, #ff9800 100%);\n",
        "        color: #ffffff;\n",
        "        padding: 1.5rem;\n",
        "        border-radius: 12px;\n",
        "        border-left: 4px solid #ffb74d;\n",
        "        margin: 1rem 0;\n",
        "        box-shadow: 0 4px 12px rgba(0,0,0,0.3);\n",
        "    }\n",
        "    .stTabs [data-baseweb=\"tab-list\"] {\n",
        "        gap: 8px;\n",
        "    }\n",
        "    .stTabs [data-baseweb=\"tab\"] {\n",
        "        background: linear-gradient(135deg, #424242 0%, #616161 100%);\n",
        "        color: #ffffff;\n",
        "        border-radius: 8px 8px 0 0;\n",
        "        padding: 12px 24px;\n",
        "        border: none;\n",
        "    }\n",
        "    .stTabs [aria-selected=\"true\"] {\n",
        "        background: linear-gradient(135deg, #1976d2 0%, #42a5f5 100%);\n",
        "        color: #ffffff;\n",
        "    }\n",
        "    .metric-card {\n",
        "        background: linear-gradient(135deg, #263238 0%, #37474f 100%);\n",
        "        padding: 1rem;\n",
        "        border-radius: 12px;\n",
        "        border: 1px solid #546e7a;\n",
        "        margin: 0.5rem 0;\n",
        "    }\n",
        "    .stSelectbox > div > div {\n",
        "        background-color: #424242;\n",
        "        color: #ffffff;\n",
        "    }\n",
        "    .stTextInput > div > div > input {\n",
        "        background-color: #424242;\n",
        "        color: #ffffff;\n",
        "        border: 1px solid #616161;\n",
        "    }\n",
        "    .stButton > button {\n",
        "        background: linear-gradient(135deg, #1976d2 0%, #42a5f5 100%);\n",
        "        color: #ffffff;\n",
        "        border: none;\n",
        "        border-radius: 8px;\n",
        "        padding: 0.5rem 1rem;\n",
        "        font-weight: bold;\n",
        "        transition: all 0.3s ease;\n",
        "    }\n",
        "    .stButton > button:hover {\n",
        "        background: linear-gradient(135deg, #1565c0 0%, #1976d2 100%);\n",
        "        box-shadow: 0 4px 12px rgba(25, 118, 210, 0.4);\n",
        "        transform: translateY(-2px);\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Initialize session state\n",
        "if 'processed_image' not in st.session_state:\n",
        "    st.session_state.processed_image = None\n",
        "if 'characters_extracted' not in st.session_state:\n",
        "    st.session_state.characters_extracted = []\n",
        "if 'character_mappings' not in st.session_state:\n",
        "    st.session_state.character_mappings = {}\n",
        "if 'processing_complete' not in st.session_state:\n",
        "    st.session_state.processing_complete = False\n",
        "if 'temp_dir' not in st.session_state:\n",
        "    st.session_state.temp_dir = tempfile.mkdtemp()\n",
        "if 'use_ocr' not in st.session_state:\n",
        "    st.session_state.use_ocr = OCR_AVAILABLE\n",
        "if 'processed_image_hash' not in st.session_state:\n",
        "    st.session_state.processed_image_hash = None\n",
        "if 'dataset_characters' not in st.session_state:\n",
        "    st.session_state.dataset_characters = {}\n",
        "if 'use_dataset' not in st.session_state:\n",
        "    st.session_state.use_dataset = False\n",
        "if 'paper_background' not in st.session_state:\n",
        "    st.session_state.paper_background = None\n",
        "if 'generated_pages' not in st.session_state:\n",
        "    st.session_state.generated_pages = []\n",
        "\n",
        "# Color definitions (same as original)\n",
        "colours = ((255, 255, 255, \"white\"),\n",
        "           (200, 100, 50, \"orange\"),\n",
        "           (128, 0, 0, \"red\"),\n",
        "           (0, 255, 0, \"green\"),\n",
        "           (0, 0, 0, \"black\"),\n",
        "           (64, 64, 64, \"grey\"))\n",
        "\n",
        "def get_image_hash(image_file):\n",
        "    \"\"\"Generate hash for uploaded image to track changes\"\"\"\n",
        "    import hashlib\n",
        "    image_file.seek(0)\n",
        "    content = image_file.read()\n",
        "    image_file.seek(0)\n",
        "    return hashlib.md5(content).hexdigest()\n",
        "\n",
        "def add_white_background_to_processed_image(image):\n",
        "    \"\"\"Add white background to processed image for better display\"\"\"\n",
        "    if image.mode == \"RGBA\":\n",
        "        # Create white background\n",
        "        white_bg = Image.new(\"RGB\", image.size, \"white\")\n",
        "        # Paste the RGBA image onto white background\n",
        "        white_bg.paste(image, mask=image.split()[-1])  # Use alpha channel as mask\n",
        "        return white_bg\n",
        "    elif image.mode == \"LA\":\n",
        "        # Handle grayscale with alpha\n",
        "        white_bg = Image.new(\"RGB\", image.size, \"white\")\n",
        "        # Convert LA to RGBA first\n",
        "        rgba_image = image.convert(\"RGBA\")\n",
        "        white_bg.paste(rgba_image, mask=rgba_image.split()[-1])\n",
        "        return white_bg\n",
        "    else:\n",
        "        # For RGB or other modes, just convert to RGB with white background\n",
        "        if image.mode != \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "        return image\n",
        "\n",
        "def generate_handwritten_text_with_background(text, char_mappings, dataset_chars, line_spacing, char_spacing, paper_bg, use_dataset=False):\n",
        "    \"\"\"Generate handwritten text with paper background\"\"\"\n",
        "    try:\n",
        "        # Combine custom and dataset characters\n",
        "        all_characters = {}\n",
        "\n",
        "        # Add dataset characters first (if using dataset)\n",
        "        if use_dataset and dataset_chars:\n",
        "            for char, img_path in dataset_chars.items():\n",
        "                try:\n",
        "                    if isinstance(img_path, str):  # If it's a path\n",
        "                        char_img = Image.open(img_path).convert(\"RGBA\")\n",
        "                    else:  # If it's already an image\n",
        "                        char_img = img_path.convert(\"RGBA\")\n",
        "\n",
        "                    # Resize lowercase characters to be smaller\n",
        "                    if char.strip().islower():\n",
        "                        target_height = int(50 * 0.7)  # 70% size for lowercase\n",
        "                        scale_factor = target_height / char_img.height\n",
        "                        new_width = int(char_img.width * scale_factor)\n",
        "                        char_img = char_img.resize((new_width, target_height), Image.Resampling.LANCZOS)\n",
        "\n",
        "                    all_characters[char.strip()] = char_img\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading dataset character {char}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        # Add custom characters (override dataset if exists)\n",
        "        for img_path, char in char_mappings.items():\n",
        "            if char.strip():\n",
        "                try:\n",
        "                    char_img = Image.open(img_path).convert(\"RGBA\")\n",
        "\n",
        "                    # Resize lowercase characters to be smaller\n",
        "                    if char.strip().islower():\n",
        "                        target_height = int(50 * 0.7)  # 70% size for lowercase\n",
        "                        scale_factor = target_height / char_img.height\n",
        "                        new_width = int(char_img.width * scale_factor)\n",
        "                        char_img = char_img.resize((new_width, target_height), Image.Resampling.LANCZOS)\n",
        "\n",
        "                    all_characters[char.strip()] = char_img\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading custom character {char}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        if not all_characters:\n",
        "            st.error(\"No character mappings available.\")\n",
        "            return None\n",
        "\n",
        "        # Split text into lines\n",
        "        lines = text.split('\\n')\n",
        "        line_images = []\n",
        "\n",
        "        for line in lines:\n",
        "            if not line.strip():\n",
        "                line_images.append(None)\n",
        "                continue\n",
        "\n",
        "            char_images = []\n",
        "            for char in line:\n",
        "                if char == ' ':\n",
        "                    # Space character - fully transparent\n",
        "                    blank = Image.new('RGBA', (char_spacing * 2, 50), (0, 0, 0, 0))\n",
        "                    char_images.append(blank)\n",
        "                elif char in all_characters:  # Check exact case first\n",
        "                    char_img = all_characters[char].copy()  # Make a copy\n",
        "                    char_images.append(char_img)\n",
        "                elif char.lower() in all_characters:  # Then check lowercase\n",
        "                    char_img = all_characters[char.lower()].copy()  # Make a copy\n",
        "                    char_images.append(char_img)\n",
        "                elif char.upper() in all_characters:  # Then check uppercase\n",
        "                    char_img = all_characters[char.upper()].copy()  # Make a copy\n",
        "                    char_images.append(char_img)\n",
        "                else:\n",
        "                    # Character not found - create placeholder\n",
        "                    placeholder = Image.new('RGBA', (20, 30), (255, 0, 0, 128))  # Red semi-transparent placeholder\n",
        "                    char_images.append(placeholder)\n",
        "\n",
        "            if char_images:\n",
        "                # Combine characters in line\n",
        "                total_width = sum(img.width for img in char_images) + char_spacing * (len(char_images) - 1)\n",
        "                max_height = max(img.height for img in char_images) if char_images else 50\n",
        "\n",
        "                line_img = Image.new('RGBA', (total_width, max_height), (255, 255, 255, 0))\n",
        "                x_offset = 0\n",
        "\n",
        "                for char_img in char_images:\n",
        "                    line_img.paste(char_img, (x_offset, 0), char_img)\n",
        "                    x_offset += char_img.width + char_spacing\n",
        "\n",
        "                line_images.append(line_img)\n",
        "            else:\n",
        "                line_images.append(None)\n",
        "\n",
        "        # Calculate total content size\n",
        "        valid_lines = [img for img in line_images if img is not None]\n",
        "        if not valid_lines:\n",
        "            st.error(\"No valid lines generated\")\n",
        "            return None\n",
        "\n",
        "        content_height = sum(img.height for img in valid_lines) + line_spacing * (len(valid_lines) - 1)\n",
        "        content_width = max(img.width for img in valid_lines)\n",
        "\n",
        "        # Use paper background or create white background\n",
        "        if paper_bg:\n",
        "            # Scale paper background to accommodate content with margins\n",
        "            margin = 100\n",
        "            required_width = content_width + 2 * margin\n",
        "            required_height = content_height + 2 * margin\n",
        "\n",
        "            # Scale background if needed\n",
        "            bg_scale_w = required_width / paper_bg.width if required_width > paper_bg.width else 1\n",
        "            bg_scale_h = required_height / paper_bg.height if required_height > paper_bg.height else 1\n",
        "            bg_scale = max(bg_scale_w, bg_scale_h)\n",
        "\n",
        "            if bg_scale > 1:\n",
        "                new_bg_width = int(paper_bg.width * bg_scale)\n",
        "                new_bg_height = int(paper_bg.height * bg_scale)\n",
        "                final_image = paper_bg.resize((new_bg_width, new_bg_height), Image.Resampling.LANCZOS)\n",
        "            else:\n",
        "                final_image = paper_bg.copy()\n",
        "\n",
        "            # Keep as RGB - don't convert to RGBA\n",
        "            if final_image.mode != \"RGB\":\n",
        "                final_image = final_image.convert(\"RGB\")\n",
        "\n",
        "            start_x = margin\n",
        "            start_y = margin\n",
        "        else:\n",
        "            # Create white background\n",
        "            final_image = Image.new('RGB', (content_width + 200, content_height + 200), (255, 255, 255))\n",
        "            start_x = 100\n",
        "            start_y = 100\n",
        "\n",
        "        # Paste text lines onto background\n",
        "        y_offset = start_y\n",
        "        for line_img in line_images:\n",
        "            if line_img is not None:\n",
        "                # Create a composite for transparency on RGB background\n",
        "                if line_img.mode == \"RGBA\":\n",
        "                    # Create temporary RGB version of the line\n",
        "                    temp_line = Image.new(\"RGB\", line_img.size, (255, 255, 255))\n",
        "                    temp_line.paste(line_img, mask=line_img.split()[-1])  # Use alpha as mask\n",
        "\n",
        "                    # Create mask from alpha channel\n",
        "                    mask = line_img.split()[-1]  # Get alpha channel\n",
        "\n",
        "                    # Paste onto final image using mask\n",
        "                    final_image.paste(temp_line, (start_x, y_offset), mask)\n",
        "                else:\n",
        "                    final_image.paste(line_img, (start_x, y_offset))\n",
        "\n",
        "                y_offset += line_img.height + line_spacing\n",
        "            else:\n",
        "                y_offset += line_spacing\n",
        "\n",
        "        return final_image\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error in text generation: {e}\")\n",
        "        import traceback\n",
        "        st.error(f\"Traceback: {traceback.format_exc()}\")\n",
        "        return None\n",
        "\n",
        "def nearest_colour(subjects, query):\n",
        "    \"\"\"Calculate the nearest colour based on RGB values\"\"\"\n",
        "    return min(subjects, key=lambda subject: sum((s - q) ** 2 for s, q in zip(subject, query)))[3]\n",
        "\n",
        "def save_character_mappings():\n",
        "    \"\"\"Save character mappings to prevent reprocessing\"\"\"\n",
        "    if hasattr(st.session_state, 'character_mappings'):\n",
        "        # This will persist in session state automatically\n",
        "        pass\n",
        "\n",
        "def load_character_mappings():\n",
        "    \"\"\"Load existing character mappings\"\"\"\n",
        "    # Mappings are already in session state\n",
        "    return st.session_state.get('character_mappings', {})\n",
        "\n",
        "def load_dataset_characters(dataset_folder=\"Dataset\"):\n",
        "    \"\"\"Load character images from dataset folder\"\"\"\n",
        "    if not os.path.exists(dataset_folder):\n",
        "        return {}\n",
        "\n",
        "    characters = {}\n",
        "\n",
        "    # Load lowercase letters (.a.png, .b.png, ...)\n",
        "    for char in 'abcdefghijklmnopqrstuvwxyz':\n",
        "        char_file = os.path.join(dataset_folder, f\".{char}.png\")\n",
        "        if os.path.exists(char_file):\n",
        "            characters[char] = char_file  # Store as lowercase key\n",
        "\n",
        "    # Load uppercase letters (A.png, B.png, ...)\n",
        "    for char in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':\n",
        "        char_file = os.path.join(dataset_folder, f\"{char}.png\")\n",
        "        if os.path.exists(char_file):\n",
        "            characters[char] = char_file  # Store as uppercase key\n",
        "\n",
        "    # Load numbers (0.png, 1.png, ...)\n",
        "    for char in '0123456789':\n",
        "        char_file = os.path.join(dataset_folder, f\"{char}.png\")\n",
        "        if os.path.exists(char_file):\n",
        "            characters[char] = char_file\n",
        "\n",
        "    # Load common punctuation\n",
        "    punctuation_map = {\n",
        "        'period': '.', 'comma': ',', 'question': '?', 'exclamation': '!',\n",
        "        'colon': ':', 'semicolon': ';', 'apostrophe': \"'\", 'quote': '\"',\n",
        "        'hyphen': '-', 'underscore': '_', 'space': ' '\n",
        "    }\n",
        "\n",
        "    for file_name, char in punctuation_map.items():\n",
        "        char_file = os.path.join(dataset_folder, f\"{file_name}.png\")\n",
        "        if os.path.exists(char_file):\n",
        "            characters[char] = char_file\n",
        "\n",
        "    return characters\n",
        "\n",
        "def load_paper_background(dataset_folder=\"Dataset\"):\n",
        "    \"\"\"Load paper background image\"\"\"\n",
        "    paper_bg_path = os.path.join(dataset_folder, \"paperbg.png\")\n",
        "    if os.path.exists(paper_bg_path):\n",
        "        bg_img = Image.open(paper_bg_path)\n",
        "        # Convert to RGB to ensure proper background handling\n",
        "        if bg_img.mode != \"RGB\":\n",
        "            bg_img = bg_img.convert(\"RGB\")\n",
        "        return bg_img\n",
        "    return None\n",
        "\n",
        "def align_character_sizes(characters_dict, target_height=50):\n",
        "    \"\"\"Align character sizes maintaining aspect ratio with lowercase adjustment\"\"\"\n",
        "    aligned_chars = {}\n",
        "\n",
        "    for char, img_path in characters_dict.items():\n",
        "        try:\n",
        "            if isinstance(img_path, str):  # If it's a path\n",
        "                img = Image.open(img_path).convert(\"RGBA\")\n",
        "            else:  # If it's already an image\n",
        "                img = img_path.convert(\"RGBA\")\n",
        "\n",
        "            # Different target heights for lowercase vs uppercase\n",
        "            if char.islower():\n",
        "                char_target_height = int(target_height * 0.7)  # 70% size for lowercase\n",
        "            else:\n",
        "                char_target_height = target_height\n",
        "\n",
        "            # Calculate scale factor\n",
        "            scale_factor = char_target_height / img.height\n",
        "            new_width = int(img.width * scale_factor)\n",
        "\n",
        "            # Resize image\n",
        "            resized_img = img.resize((new_width, char_target_height), Image.Resampling.LANCZOS)\n",
        "            aligned_chars[char] = resized_img\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error processing character {char}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return aligned_chars\n",
        "\n",
        "def transcribe_audio(audio_file):\n",
        "    \"\"\"Transcribe audio file to text using Speech2Text\"\"\"\n",
        "    if not STT_AVAILABLE:\n",
        "        return \"Speech-to-Text not available\"\n",
        "\n",
        "    try:\n",
        "        model, processor = load_speech_model()\n",
        "\n",
        "        # Read audio file\n",
        "        audio_data, sample_rate = sf.read(audio_file)\n",
        "\n",
        "        # Process audio\n",
        "        inputs = processor(\n",
        "            audio_data,\n",
        "            sampling_rate=sample_rate,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Generate transcription\n",
        "        generated_ids = model.generate(\n",
        "            inputs[\"input_features\"],\n",
        "            attention_mask=inputs[\"attention_mask\"]\n",
        "        )\n",
        "\n",
        "        # Decode transcription\n",
        "        transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        return transcription[0] if transcription else \"No transcription available\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error transcribing audio: {str(e)}\"\n",
        "\n",
        "def create_directories():\n",
        "    \"\"\"Create necessary directories\"\"\"\n",
        "    data_dir = os.path.join(st.session_state.temp_dir, \"data\")\n",
        "    characters_dir = os.path.join(data_dir, \"characters\")\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    os.makedirs(characters_dir, exist_ok=True)\n",
        "    return data_dir, characters_dir\n",
        "\n",
        "def process_image_advanced(image_file):\n",
        "    \"\"\"Enhanced version of the original main function with progress tracking\"\"\"\n",
        "    data_dir, characters_dir = create_directories()\n",
        "\n",
        "    # Save uploaded file temporarily\n",
        "    temp_image_path = os.path.join(st.session_state.temp_dir, \"input_image.png\")\n",
        "    with open(temp_image_path, \"wb\") as f:\n",
        "        f.write(image_file.getvalue())\n",
        "\n",
        "    progress_bar = st.progress(0)\n",
        "    status_text = st.empty()\n",
        "\n",
        "    try:\n",
        "        # Step 1: Load and enhance image\n",
        "        status_text.text(\"Loading and enhancing image...\")\n",
        "        progress_bar.progress(10)\n",
        "\n",
        "        original_picture = Image.open(temp_image_path)\n",
        "        width, height = original_picture.size\n",
        "\n",
        "        # Enhanced upscaling with better resampling\n",
        "        picture = original_picture.resize((width * 2, height * 2), Image.Resampling.LANCZOS)\n",
        "        original_picture.close()\n",
        "        width, height = picture.size\n",
        "\n",
        "        # Step 2: Background removal\n",
        "        status_text.text(\"Removing background and isolating text...\")\n",
        "        progress_bar.progress(25)\n",
        "\n",
        "        rgba_image = picture.convert(\"RGBA\")\n",
        "        picture.close()\n",
        "\n",
        "        for y in range(height):\n",
        "            for x in range(width):\n",
        "                r, g, b, a = rgba_image.getpixel((x, y))\n",
        "                if not nearest_colour(colours, (r, g, b)) in [\"black\", \"grey\"]:\n",
        "                    rgba_image.putpixel((x, y), (255, 255, 255, 0))\n",
        "                else:\n",
        "                    rgba_image.putpixel((x, y), (0, 0, 0, 255))\n",
        "\n",
        "        # Step 3: Image enhancement\n",
        "        status_text.text(\"Enhancing image quality...\")\n",
        "        progress_bar.progress(40)\n",
        "\n",
        "        detail = rgba_image.filter(ImageFilter.EDGE_ENHANCE)\n",
        "        rgba_image.close()\n",
        "        smooth = detail.filter(ImageFilter.SMOOTH)\n",
        "        sharpen = smooth.filter(ImageFilter.SHARPEN)\n",
        "        smooth.close()\n",
        "\n",
        "        alpha_path = os.path.join(data_dir, \"AlphabetAlpha.png\")\n",
        "        sharpen.save(alpha_path)\n",
        "        # Add white background for display\n",
        "        st.session_state.processed_image = add_white_background_to_processed_image(sharpen.copy())\n",
        "        sharpen.close()\n",
        "\n",
        "        # Step 4: Vertical cropping\n",
        "        status_text.text(\"Segmenting text rows...\")\n",
        "        progress_bar.progress(55)\n",
        "\n",
        "        rgba_alphabet = Image.open(alpha_path)\n",
        "        ycroplist = []\n",
        "\n",
        "        for y in range(height):\n",
        "            no_letters = True\n",
        "            for x in range(width):\n",
        "                r, g, b, a = rgba_alphabet.getpixel((x, y))\n",
        "                if nearest_colour(colours, (r, g, b)) in [\"black\", \"grey\"]:\n",
        "                    no_letters = False\n",
        "                    break\n",
        "            if no_letters:\n",
        "                ycroplist.append(y)\n",
        "\n",
        "        # Create vertical crops\n",
        "        vertical_counter = 1\n",
        "        for y in range(len(ycroplist) - 1):\n",
        "            if ycroplist[y] + 1 != ycroplist[y + 1]:\n",
        "                cropped = rgba_alphabet.crop((0, ycroplist[y] + 1, width, ycroplist[y + 1]))\n",
        "                cropped.save(os.path.join(data_dir, f\"VerticalCrop{vertical_counter}.png\"))\n",
        "                cropped.close()\n",
        "                vertical_counter += 1\n",
        "\n",
        "        rgba_alphabet.close()\n",
        "\n",
        "        # Step 5: Horizontal cropping\n",
        "        status_text.text(\"Segmenting individual characters...\")\n",
        "        progress_bar.progress(70)\n",
        "\n",
        "        files = os.listdir(data_dir)\n",
        "        horizontal_counter = 1\n",
        "\n",
        "        for item in files:\n",
        "            if \"VerticalCrop\" in item:\n",
        "                rgba_alphabet = Image.open(os.path.join(data_dir, item))\n",
        "                width_crop, height_crop = rgba_alphabet.size\n",
        "                xcroplist = []\n",
        "\n",
        "                for x in range(width_crop):\n",
        "                    no_letters = True\n",
        "                    for y in range(height_crop):\n",
        "                        r, g, b, a = rgba_alphabet.getpixel((x, y))\n",
        "                        if nearest_colour(colours, (r, g, b)) in [\"black\", \"grey\"]:\n",
        "                            no_letters = False\n",
        "                            break\n",
        "                    if no_letters:\n",
        "                        xcroplist.append(x)\n",
        "\n",
        "                for x in range(len(xcroplist) - 1):\n",
        "                    if xcroplist[x] + 1 != xcroplist[x + 1]:\n",
        "                        cropped = rgba_alphabet.crop((xcroplist[x] + 1, 0, xcroplist[x + 1], height_crop))\n",
        "                        cropped.save(os.path.join(data_dir, f\"HorizontalCrop{horizontal_counter}.png\"))\n",
        "                        cropped.close()\n",
        "                        horizontal_counter += 1\n",
        "\n",
        "                rgba_alphabet.close()\n",
        "\n",
        "        # Step 6: Final character cropping\n",
        "        status_text.text(\"Finalizing character extraction...\")\n",
        "        progress_bar.progress(85)\n",
        "\n",
        "        files = os.listdir(data_dir)\n",
        "        counter = 0\n",
        "        extracted_characters = []\n",
        "\n",
        "        for item in files:\n",
        "            if \"HorizontalCrop\" in item:\n",
        "                counter += 1\n",
        "                rgba_character = Image.open(os.path.join(data_dir, item))\n",
        "                width_char, height_char = rgba_character.size\n",
        "                croplist = []\n",
        "\n",
        "                for y in range(height_char):\n",
        "                    for x in range(width_char):\n",
        "                        r, g, b, a = rgba_character.getpixel((x, y))\n",
        "                        if nearest_colour(colours, (r, g, b)) in [\"black\", \"grey\"]:\n",
        "                            croplist.append(y)\n",
        "\n",
        "                if croplist:\n",
        "                    cropped = rgba_character.crop((0, croplist[0], width_char, croplist[-1]))\n",
        "                    char_path = os.path.join(characters_dir, f\"Cropped{counter}.png\")\n",
        "                    cropped.save(char_path)\n",
        "                    extracted_characters.append(char_path)\n",
        "                    cropped.close()\n",
        "\n",
        "                rgba_character.close()\n",
        "                os.remove(os.path.join(data_dir, item))\n",
        "\n",
        "        # Cleanup\n",
        "        for item in files:\n",
        "            if \"VerticalCrop\" in item:\n",
        "                try:\n",
        "                    os.remove(os.path.join(data_dir, item))\n",
        "                except FileNotFoundError:\n",
        "                    pass\n",
        "\n",
        "        try:\n",
        "            os.remove(alpha_path)\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "        status_text.text(\"Processing complete!\")\n",
        "        progress_bar.progress(100)\n",
        "\n",
        "        st.session_state.characters_extracted = extracted_characters\n",
        "        st.session_state.processing_complete = True\n",
        "\n",
        "        return True, f\"Successfully extracted {len(extracted_characters)} characters\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return False, f\"Error during processing: {str(e)}\"\n",
        "\n",
        "def prepare_character_for_display(char_path, target_size=(120, 120), padding=10):\n",
        "    \"\"\"Prepare character image for uniform display with white background\"\"\"\n",
        "    try:\n",
        "        # Load the character image\n",
        "        char_img = Image.open(char_path).convert(\"RGBA\")\n",
        "\n",
        "        # Calculate scaling to fit within target size while maintaining aspect ratio\n",
        "        original_width, original_height = char_img.size\n",
        "        scale_factor = min(\n",
        "            (target_size[0] - 2 * padding) / original_width,\n",
        "            (target_size[1] - 2 * padding) / original_height\n",
        "        )\n",
        "\n",
        "        new_width = int(original_width * scale_factor)\n",
        "        new_height = int(original_height * scale_factor)\n",
        "\n",
        "        # Resize the character\n",
        "        resized_char = char_img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "\n",
        "        # Create white background\n",
        "        display_img = Image.new(\"RGB\", target_size, \"white\")\n",
        "\n",
        "        # Calculate position to center the character\n",
        "        x_offset = (target_size[0] - new_width) // 2\n",
        "        y_offset = (target_size[1] - new_height) // 2\n",
        "\n",
        "        # Paste the character onto white background\n",
        "        if resized_char.mode == \"RGBA\":\n",
        "            # Handle transparency\n",
        "            display_img.paste(resized_char, (x_offset, y_offset), resized_char)\n",
        "        else:\n",
        "            display_img.paste(resized_char, (x_offset, y_offset))\n",
        "\n",
        "        # Add border\n",
        "        draw = ImageDraw.Draw(display_img)\n",
        "        draw.rectangle([0, 0, target_size[0]-1, target_size[1]-1], outline=\"lightgray\", width=1)\n",
        "\n",
        "        return display_img\n",
        "    except Exception as e:\n",
        "        # Return error placeholder\n",
        "        error_img = Image.new(\"RGB\", target_size, \"white\")\n",
        "        draw = ImageDraw.Draw(error_img)\n",
        "        draw.text((10, target_size[1]//2), \"Error\", fill=\"red\")\n",
        "        return error_img\n",
        "\n",
        "def run_ocr_on_character(char_path):\n",
        "    \"\"\"Run OCR on a single character image\"\"\"\n",
        "    if not OCR_AVAILABLE:\n",
        "        return \"N/A\"\n",
        "\n",
        "    try:\n",
        "        ocr = load_ocr_model()\n",
        "        # Prepare image for OCR\n",
        "        img = Image.open(char_path).convert(\"RGB\")\n",
        "\n",
        "        # Enhance image for better OCR\n",
        "        img = img.resize((img.width * 2, img.height * 2), Image.Resampling.LANCZOS)\n",
        "\n",
        "        # Run OCR\n",
        "        result = ocr(img)\n",
        "        if result and len(result) > 0:\n",
        "            predicted_text = result[0]['generated_text'].strip()\n",
        "            # Clean up the result (take first character if multiple)\n",
        "            if predicted_text:\n",
        "                return predicted_text[0].upper() if predicted_text else \"?\"\n",
        "        return \"?\"\n",
        "    except Exception as e:\n",
        "        return \"Error\"\n",
        "\n",
        "def display_character_grid(characters, cols_per_row=6, show_ocr=False):\n",
        "    \"\"\"Display characters in an equal grid layout with uniform sizing\"\"\"\n",
        "    if not characters:\n",
        "        st.info(\"No characters to display\")\n",
        "        return\n",
        "\n",
        "    st.write(f\"**Total Characters Extracted:** {len(characters)}\")\n",
        "\n",
        "    # OCR toggle\n",
        "    if OCR_AVAILABLE and show_ocr:\n",
        "        with st.spinner(\"Running OCR on characters...\"):\n",
        "            ocr_results = {}\n",
        "            progress_bar = st.progress(0)\n",
        "            for idx, char_path in enumerate(characters):\n",
        "                ocr_results[char_path] = run_ocr_on_character(char_path)\n",
        "                progress_bar.progress((idx + 1) / len(characters))\n",
        "            progress_bar.empty()\n",
        "\n",
        "    # Create grid layout\n",
        "    rows = [characters[i:i + cols_per_row] for i in range(0, len(characters), cols_per_row)]\n",
        "\n",
        "    for row_idx, row in enumerate(rows):\n",
        "        # Create columns with equal width\n",
        "        cols = st.columns(cols_per_row)\n",
        "\n",
        "        for col_idx, char_path in enumerate(row):\n",
        "            with cols[col_idx]:\n",
        "                try:\n",
        "                    # Prepare uniform character image\n",
        "                    display_img = prepare_character_for_display(char_path)\n",
        "\n",
        "                    # Create caption\n",
        "                    char_num = row_idx * cols_per_row + col_idx + 1\n",
        "                    caption = f\"#{char_num}\"\n",
        "\n",
        "                    if OCR_AVAILABLE and show_ocr and char_path in ocr_results:\n",
        "                        caption += f\" | OCR: {ocr_results[char_path]}\"\n",
        "\n",
        "                    st.image(display_img, caption=caption, use_container_width=True)\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error displaying character {col_idx + 1}: {str(e)}\")\n",
        "\n",
        "        # Fill empty columns in the last row\n",
        "        if len(row) < cols_per_row:\n",
        "            for empty_col in range(len(row), cols_per_row):\n",
        "                with cols[empty_col]:\n",
        "                    st.empty()\n",
        "\n",
        "def create_character_mapping_interface():\n",
        "    \"\"\"Create an interface for mapping characters with OCR assistance\"\"\"\n",
        "    st.markdown('<div class=\"step-header\">Character Verification & Mapping</div>', unsafe_allow_html=True)\n",
        "\n",
        "    if not st.session_state.characters_extracted:\n",
        "        st.warning(\"No characters extracted yet. Please process an image first.\")\n",
        "        return\n",
        "\n",
        "    # OCR options\n",
        "    col1, col2, col3 = st.columns([2, 1, 1])\n",
        "    with col1:\n",
        "        st.markdown(\"**Options:**\")\n",
        "    with col2:\n",
        "        auto_ocr = st.checkbox(\"Enable OCR Assistance\", value=OCR_AVAILABLE, disabled=not OCR_AVAILABLE)\n",
        "    with col3:\n",
        "        if auto_ocr and st.button(\"Run OCR on All\", type=\"secondary\"):\n",
        "            with st.spinner(\"Running OCR...\"):\n",
        "                for char_path in st.session_state.characters_extracted:\n",
        "                    if char_path not in st.session_state.character_mappings or not st.session_state.character_mappings[char_path]:\n",
        "                        ocr_result = run_ocr_on_character(char_path)\n",
        "                        st.session_state.character_mappings[char_path] = ocr_result\n",
        "                st.success(\"OCR completed!\")\n",
        "                st.rerun()\n",
        "\n",
        "    # Character mapping form\n",
        "    with st.form(\"character_mapping\"):\n",
        "        st.write(\"Map each extracted character to its actual character:\")\n",
        "\n",
        "        # Grid layout for mapping\n",
        "        cols_per_row = 4\n",
        "        rows = [st.session_state.characters_extracted[i:i + cols_per_row]\n",
        "                for i in range(0, len(st.session_state.characters_extracted), cols_per_row)]\n",
        "\n",
        "        mappings = {}\n",
        "\n",
        "        for row_idx, row in enumerate(rows):\n",
        "            cols = st.columns(cols_per_row)\n",
        "\n",
        "            for col_idx, char_path in enumerate(row):\n",
        "                with cols[col_idx]:\n",
        "                    try:\n",
        "                        # Display character with uniform sizing\n",
        "                        display_img = prepare_character_for_display(char_path, target_size=(100, 100))\n",
        "                        char_num = row_idx * cols_per_row + col_idx + 1\n",
        "                        st.image(display_img, caption=f\"Character #{char_num}\", use_container_width=True)\n",
        "\n",
        "                        # Get existing mapping or OCR suggestion\n",
        "                        existing_mapping = st.session_state.character_mappings.get(char_path, \"\")\n",
        "\n",
        "                        # OCR suggestion\n",
        "                        ocr_suggestion = \"\"\n",
        "                        if auto_ocr and OCR_AVAILABLE:\n",
        "                            ocr_suggestion = run_ocr_on_character(char_path)\n",
        "                            if not existing_mapping and ocr_suggestion != \"Error\":\n",
        "                                existing_mapping = ocr_suggestion\n",
        "\n",
        "                        char_input = st.text_input(\n",
        "                            f\"Char #{char_num}:\",\n",
        "                            value=existing_mapping,\n",
        "                            key=f\"char_{char_num}\",\n",
        "                            max_chars=5,\n",
        "                            placeholder=f\"OCR: {ocr_suggestion}\" if ocr_suggestion else \"Enter character\"\n",
        "                        )\n",
        "                        mappings[char_path] = char_input\n",
        "\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"Error displaying character {char_num}: {e}\")\n",
        "\n",
        "            # Fill empty columns in the last row\n",
        "            if len(row) < cols_per_row:\n",
        "                for empty_col in range(len(row), cols_per_row):\n",
        "                    with cols[empty_col]:\n",
        "                        st.empty()\n",
        "\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            if st.form_submit_button(\"Save Character Mappings\", type=\"primary\"):\n",
        "                st.session_state.character_mappings = mappings\n",
        "                st.success(\"Character mappings saved successfully!\")\n",
        "\n",
        "                # Display mapping summary\n",
        "                st.write(\"### Mapping Summary:\")\n",
        "                mapping_data = []\n",
        "                for idx, (char_path, mapping) in enumerate(mappings.items(), 1):\n",
        "                    if mapping.strip():\n",
        "                        mapping_data.append({\n",
        "                            \"Character #\": idx,\n",
        "                            \"File\": os.path.basename(char_path),\n",
        "                            \"Mapped To\": mapping.strip()\n",
        "                        })\n",
        "\n",
        "                if mapping_data:\n",
        "                    df = pd.DataFrame(mapping_data)\n",
        "                    st.dataframe(df, use_container_width=True)\n",
        "\n",
        "        with col2:\n",
        "            if st.form_submit_button(\"Clear All Mappings\", type=\"secondary\"):\n",
        "                st.session_state.character_mappings = {}\n",
        "                st.warning(\"All mappings cleared!\")\n",
        "                st.rerun()\n",
        "\n",
        "def create_text_generation_interface():\n",
        "    \"\"\"Interface for generating handwritten text with dataset support\"\"\"\n",
        "    st.markdown('<div class=\"step-header\">Text Generation</div>', unsafe_allow_html=True)\n",
        "\n",
        "    # Load dataset characters if not already loaded\n",
        "    if not st.session_state.dataset_characters:\n",
        "        st.session_state.dataset_characters = load_dataset_characters()\n",
        "        st.session_state.paper_background = load_paper_background()\n",
        "\n",
        "    # Dataset status\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "        if st.session_state.dataset_characters:\n",
        "            st.success(f\"✅ Dataset loaded ({len(st.session_state.dataset_characters)} characters)\")\n",
        "        else:\n",
        "            st.warning(\"⚠️ No dataset found in 'Dataset' folder\")\n",
        "\n",
        "    with col2:\n",
        "        if st.session_state.paper_background:\n",
        "            st.success(\"✅ Paper background loaded\")\n",
        "        else:\n",
        "            st.info(\"📄 No paperbg.png found\")\n",
        "\n",
        "    with col3:\n",
        "        st.session_state.use_dataset = st.checkbox(\n",
        "            \"Use dataset characters\",\n",
        "            value=st.session_state.use_dataset,\n",
        "            help=\"Use dataset characters for missing letters\"\n",
        "        )\n",
        "\n",
        "    # Input options\n",
        "    st.markdown(\"### Text Input Options\")\n",
        "    input_method = st.radio(\n",
        "        \"Choose input method:\",\n",
        "        [\"Type Text\", \"Upload Text File\", \"Speech-to-Text\"],\n",
        "        horizontal=True\n",
        "    )\n",
        "\n",
        "    input_text = \"\"\n",
        "\n",
        "    if input_method == \"Type Text\":\n",
        "        input_text = st.text_area(\n",
        "            \"Enter text to convert to handwriting:\",\n",
        "            height=150,\n",
        "            placeholder=\"Type your text here...\"\n",
        "        )\n",
        "\n",
        "    elif input_method == \"Upload Text File\":\n",
        "        uploaded_text_file = st.file_uploader(\n",
        "            \"Upload a text file\",\n",
        "            type=['txt'],\n",
        "            help=\"Upload a .txt file with your content\"\n",
        "        )\n",
        "\n",
        "        if uploaded_text_file is not None:\n",
        "            try:\n",
        "                input_text = uploaded_text_file.read().decode('utf-8')\n",
        "                st.text_area(\n",
        "                    \"File content preview:\",\n",
        "                    value=input_text,\n",
        "                    height=150,\n",
        "                    disabled=True\n",
        "                )\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error reading file: {e}\")\n",
        "                return\n",
        "\n",
        "    elif input_method == \"Speech-to-Text\":\n",
        "        if STT_AVAILABLE:\n",
        "            uploaded_audio = st.file_uploader(\n",
        "                \"Upload audio file\",\n",
        "                type=['wav', 'flac', 'mp3'],\n",
        "                help=\"Upload an audio file to transcribe\"\n",
        "            )\n",
        "\n",
        "            if uploaded_audio is not None:\n",
        "                with st.spinner(\"Transcribing audio...\"):\n",
        "                    transcription = transcribe_audio(uploaded_audio)\n",
        "\n",
        "                input_text = st.text_area(\n",
        "                    \"Transcribed text:\",\n",
        "                    value=transcription,\n",
        "                    height=150,\n",
        "                    help=\"Edit the transcription if needed\"\n",
        "                )\n",
        "        else:\n",
        "            st.error(\"Speech-to-Text not available. Install required packages.\")\n",
        "            return\n",
        "\n",
        "    if not input_text.strip():\n",
        "        st.info(\"Please enter text, upload a file, or provide audio to continue.\")\n",
        "        return\n",
        "\n",
        "    # Generation options\n",
        "    st.markdown(\"### Generation Settings\")\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        line_spacing = st.slider(\"Line Spacing\", 1, 50, 20, key=\"line_spacing_gen\")\n",
        "        char_spacing = st.slider(\"Character Spacing\", 1, 20, 5, key=\"char_spacing_gen\")\n",
        "        use_paper_bg = st.checkbox(\"Use paper background\", value=bool(st.session_state.paper_background), key=\"use_paper_bg_gen\")\n",
        "\n",
        "    with col2:\n",
        "        output_width = st.slider(\"Output Width\", 500, 2000, 1000, key=\"output_width_gen\")\n",
        "        output_format = st.selectbox(\"Output Format\", [\"PNG\", \"JPEG\"], key=\"output_format_gen\")\n",
        "        page_title = st.text_input(\"Page Title (optional)\", placeholder=\"Page 1\", key=\"page_title_gen\")\n",
        "\n",
        "    # Generate button\n",
        "    if st.button(\"Generate Page\", type=\"primary\"):\n",
        "        if not st.session_state.character_mappings and not st.session_state.use_dataset:\n",
        "            st.warning(\"Please complete character mapping first or enable dataset usage.\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Align dataset characters if using dataset\n",
        "            aligned_dataset = {}\n",
        "            if st.session_state.use_dataset and st.session_state.dataset_characters:\n",
        "                aligned_dataset = align_character_sizes(st.session_state.dataset_characters)\n",
        "\n",
        "            paper_bg = st.session_state.paper_background if use_paper_bg else None\n",
        "\n",
        "            generated_image = generate_handwritten_text_with_background(\n",
        "                input_text,\n",
        "                st.session_state.character_mappings,\n",
        "                aligned_dataset,  # Pass aligned dataset, not raw dataset\n",
        "                line_spacing,\n",
        "                char_spacing,\n",
        "                paper_bg,\n",
        "                st.session_state.use_dataset\n",
        "            )\n",
        "\n",
        "            if generated_image:\n",
        "                st.image(generated_image, caption=f\"Generated: {page_title or 'Handwritten Text'}\")\n",
        "\n",
        "                # Add to pages\n",
        "                page_data = {\n",
        "                    'image': generated_image,\n",
        "                    'title': page_title or f\"Page {len(st.session_state.generated_pages) + 1}\",\n",
        "                    'text_preview': input_text[:100] + \"...\" if len(input_text) > 100 else input_text\n",
        "                }\n",
        "                st.session_state.generated_pages.append(page_data)\n",
        "\n",
        "                # Download options\n",
        "                col1, col2, col3 = st.columns(3)\n",
        "\n",
        "                with col1:\n",
        "                    # Download single image\n",
        "                    img_buffer = BytesIO()\n",
        "                    generated_image.save(img_buffer, format=output_format)\n",
        "                    img_buffer.seek(0)\n",
        "\n",
        "                    st.download_button(\n",
        "                        label=f\"📥 Download {output_format}\",\n",
        "                        data=img_buffer,\n",
        "                        file_name=f\"handwritten_{page_title or 'text'}.{output_format.lower()}\",\n",
        "                        mime=f\"image/{output_format.lower()}\"\n",
        "                    )\n",
        "\n",
        "                with col2:\n",
        "                    # Download single page PDF\n",
        "                    if st.session_state.generated_pages:\n",
        "                        temp_pdf_path = os.path.join(st.session_state.temp_dir, \"temp_single.pdf\")\n",
        "\n",
        "                        if create_pdf_from_images([generated_image], temp_pdf_path):\n",
        "                            with open(temp_pdf_path, \"rb\") as f:\n",
        "                                pdf_data = f.read()\n",
        "\n",
        "                            st.download_button(\n",
        "                                label=\"📄 Download PDF\",\n",
        "                                data=pdf_data,\n",
        "                                file_name=f\"handwritten_{page_title or 'text'}.pdf\",\n",
        "                                mime=\"application/pdf\"\n",
        "                            )\n",
        "\n",
        "                with col3:\n",
        "                    st.success(f\"✅ Page added ({len(st.session_state.generated_pages)} total)\")\n",
        "\n",
        "            else:\n",
        "                st.error(\"Failed to generate handwritten text.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error generating text: {e}\")\n",
        "\n",
        "    # Multi-page management\n",
        "    if st.session_state.generated_pages:\n",
        "        st.markdown(\"---\")\n",
        "        st.markdown(\"### Generated Pages\")\n",
        "\n",
        "        # Show all pages\n",
        "        for idx, page_data in enumerate(st.session_state.generated_pages):\n",
        "            with st.expander(f\"📄 {page_data['title']}\", expanded=False):\n",
        "                col1, col2 = st.columns([3, 1])\n",
        "                with col1:\n",
        "                    st.image(page_data['image'], use_container_width=True)\n",
        "                with col2:\n",
        "                    st.write(\"**Preview:**\")\n",
        "                    st.write(page_data['text_preview'])\n",
        "                    if st.button(f\"🗑️ Remove\", key=f\"remove_{idx}\"):\n",
        "                        st.session_state.generated_pages.pop(idx)\n",
        "                        st.rerun()\n",
        "\n",
        "        # Multi-page PDF download\n",
        "        st.markdown(\"### Combined PDF\")\n",
        "        col1, col2, col3 = st.columns([1, 1, 2])\n",
        "\n",
        "        with col1:\n",
        "            if st.button(\"📚 Generate Combined PDF\", type=\"primary\"):\n",
        "                try:\n",
        "                    temp_pdf_path = os.path.join(st.session_state.temp_dir, \"combined_pages.pdf\")\n",
        "                    images = [page['image'] for page in st.session_state.generated_pages]\n",
        "\n",
        "                    if create_pdf_from_images(images, temp_pdf_path):\n",
        "                        with open(temp_pdf_path, \"rb\") as f:\n",
        "                            pdf_data = f.read()\n",
        "\n",
        "                        st.session_state.combined_pdf = pdf_data\n",
        "                        st.success(\"Combined PDF ready!\")\n",
        "                    else:\n",
        "                        st.error(\"Failed to create combined PDF\")\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error creating combined PDF: {e}\")\n",
        "\n",
        "        with col2:\n",
        "            if hasattr(st.session_state, 'combined_pdf'):\n",
        "                st.download_button(\n",
        "                    label=\"📥 Download Combined PDF\",\n",
        "                    data=st.session_state.combined_pdf,\n",
        "                    file_name=\"handwritten_document.pdf\",\n",
        "                    mime=\"application/pdf\"\n",
        "                )\n",
        "\n",
        "        with col3:\n",
        "            if st.button(\"🗑️ Clear All Pages\"):\n",
        "                st.session_state.generated_pages = []\n",
        "                if hasattr(st.session_state, 'combined_pdf'):\n",
        "                    del st.session_state.combined_pdf\n",
        "                st.rerun()\n",
        "\n",
        "    if not input_text.strip():\n",
        "        st.info(\"Please enter text or upload a file to continue.\")\n",
        "        return\n",
        "\n",
        "def generate_handwritten_text(text, char_mappings, line_spacing, char_spacing, output_width):\n",
        "    \"\"\"Generate handwritten text from character mappings\"\"\"\n",
        "    try:\n",
        "        # Add custom characters (override dataset if exists)\n",
        "        reverse_mapping = {}\n",
        "        for img_path, char in char_mappings.items():\n",
        "            if char.strip():\n",
        "                try:\n",
        "                    char_img = Image.open(img_path).convert(\"RGBA\")\n",
        "                    all_characters[char.strip()] = char_img  # Keep original case\n",
        "                    reverse_mapping[char.strip()] = img_path\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "        # Split text into lines\n",
        "        lines = text.split('\\n')\n",
        "        line_images = []\n",
        "\n",
        "        for line in lines:\n",
        "            if not line.strip():\n",
        "                # Empty line\n",
        "                line_images.append(None)\n",
        "                continue\n",
        "\n",
        "            char_images = []\n",
        "            for char in line.lower():\n",
        "                if char == ' ':\n",
        "                    # Space character - create blank space\n",
        "                    blank = Image.new('RGBA', (char_spacing * 2, 50), (255, 255, 255, 0))\n",
        "                    char_images.append(blank)\n",
        "                elif char in reverse_mapping:\n",
        "                    try:\n",
        "                        char_img = Image.open(reverse_mapping[char])\n",
        "                        char_images.append(char_img)\n",
        "                    except Exception:\n",
        "                        # If character image can't be loaded, skip\n",
        "                        continue\n",
        "                else:\n",
        "                    # Character not found - create placeholder\n",
        "                    placeholder = Image.new('RGBA', (20, 30), (255, 255, 255, 0))\n",
        "                    char_images.append(placeholder)\n",
        "\n",
        "            if char_images:\n",
        "                # Combine characters in line\n",
        "                total_width = sum(img.width for img in char_images) + char_spacing * (len(char_images) - 1)\n",
        "                max_height = max(img.height for img in char_images) if char_images else 50\n",
        "\n",
        "                line_img = Image.new('RGBA', (min(total_width, output_width), max_height), (255, 255, 255, 0))\n",
        "                x_offset = 0\n",
        "\n",
        "                for char_img in char_images:\n",
        "                    if x_offset + char_img.width <= output_width:\n",
        "                        line_img.paste(char_img, (x_offset, 0), char_img)\n",
        "                        x_offset += char_img.width + char_spacing\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                line_images.append(line_img)\n",
        "            else:\n",
        "                line_images.append(None)\n",
        "\n",
        "        # Combine all lines\n",
        "        valid_lines = [img for img in line_images if img is not None]\n",
        "        if not valid_lines:\n",
        "            return None\n",
        "\n",
        "        total_height = sum(img.height for img in valid_lines) + line_spacing * (len(valid_lines) - 1)\n",
        "        final_width = max(img.width for img in valid_lines)\n",
        "\n",
        "        final_image = Image.new('RGBA', (final_width, total_height), (255, 255, 255, 255))\n",
        "        y_offset = 0\n",
        "\n",
        "        for line_img in line_images:\n",
        "            if line_img is not None:\n",
        "                final_image.paste(line_img, (0, y_offset), line_img)\n",
        "                y_offset += line_img.height + line_spacing\n",
        "            else:\n",
        "                y_offset += line_spacing\n",
        "\n",
        "        return final_image\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error in text generation: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_analytics_dashboard():\n",
        "    \"\"\"Create analytics dashboard\"\"\"\n",
        "    st.markdown('<div class=\"step-header\">Analytics Dashboard</div>', unsafe_allow_html=True)\n",
        "\n",
        "    if not st.session_state.characters_extracted:\n",
        "        st.info(\"No data available. Process an image first.\")\n",
        "        return\n",
        "\n",
        "    # Character statistics\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "\n",
        "    with col1:\n",
        "        st.metric(\"Total Characters Extracted\", len(st.session_state.characters_extracted))\n",
        "\n",
        "    with col2:\n",
        "        mapped_chars = len([v for v in st.session_state.character_mappings.values() if v.strip()])\n",
        "        st.metric(\"Characters Mapped\", mapped_chars)\n",
        "\n",
        "    with col3:\n",
        "        completion_rate = (mapped_chars / len(st.session_state.characters_extracted) * 100) if st.session_state.characters_extracted else 0\n",
        "        st.metric(\"Completion Rate\", f\"{completion_rate:.1f}%\")\n",
        "\n",
        "    # Character size analysis\n",
        "    if st.session_state.characters_extracted:\n",
        "        st.write(\"### Character Size Analysis\")\n",
        "\n",
        "        sizes = []\n",
        "        for char_path in st.session_state.characters_extracted:\n",
        "            try:\n",
        "                img = Image.open(char_path)\n",
        "                sizes.append({\n",
        "                    'Character': os.path.basename(char_path),\n",
        "                    'Width': img.width,\n",
        "                    'Height': img.height,\n",
        "                    'Area': img.width * img.height\n",
        "                })\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        if sizes:\n",
        "            df_sizes = pd.DataFrame(sizes)\n",
        "\n",
        "            fig_scatter = px.scatter(\n",
        "                df_sizes,\n",
        "                x='Width',\n",
        "                y='Height',\n",
        "                hover_data=['Character', 'Area'],\n",
        "                title=\"Character Dimensions\",\n",
        "                color='Area'\n",
        "            )\n",
        "            st.plotly_chart(fig_scatter, use_container_width=True)\n",
        "\n",
        "            # Size distribution\n",
        "            fig_hist = px.histogram(\n",
        "                df_sizes,\n",
        "                x='Area',\n",
        "                title=\"Character Size Distribution\",\n",
        "                nbins=20\n",
        "            )\n",
        "            st.plotly_chart(fig_hist, use_container_width=True)\n",
        "\n",
        "def export_project():\n",
        "    \"\"\"Export project data with fixed download buttons\"\"\"\n",
        "    st.markdown('<div class=\"step-header\">Export Project</div>', unsafe_allow_html=True)\n",
        "\n",
        "    if not st.session_state.characters_extracted and not st.session_state.generated_pages:\n",
        "        st.warning(\"No project data to export.\")\n",
        "        return\n",
        "\n",
        "    export_format = st.selectbox(\n",
        "        \"Choose export format:\",\n",
        "        [\"ZIP Archive\", \"JSON Mappings\", \"Character Images Only\", \"Generated Pages Only\"]\n",
        "    )\n",
        "\n",
        "    if st.button(\"Prepare Export\", type=\"primary\"):\n",
        "        try:\n",
        "            if export_format == \"ZIP Archive\":\n",
        "                # Create ZIP with all project files\n",
        "                zip_buffer = BytesIO()\n",
        "                with zipfile.ZipFile(zip_buffer, 'w') as zip_file:\n",
        "                    # Add character images\n",
        "                    for char_path in st.session_state.characters_extracted:\n",
        "                        if os.path.exists(char_path):\n",
        "                            zip_file.write(char_path, f\"characters/{os.path.basename(char_path)}\")\n",
        "\n",
        "                    # Add mappings as JSON\n",
        "                    mappings_json = json.dumps(st.session_state.character_mappings, indent=2)\n",
        "                    zip_file.writestr(\"character_mappings.json\", mappings_json)\n",
        "\n",
        "                    # Add processed image if available\n",
        "                    if st.session_state.processed_image:\n",
        "                        img_buffer = BytesIO()\n",
        "                        st.session_state.processed_image.save(img_buffer, format='PNG')\n",
        "                        zip_file.writestr(\"processed_image.png\", img_buffer.getvalue())\n",
        "\n",
        "                    # Add generated pages\n",
        "                    for idx, page_data in enumerate(st.session_state.generated_pages):\n",
        "                        page_buffer = BytesIO()\n",
        "                        page_data['image'].save(page_buffer, format='PNG')\n",
        "                        zip_file.writestr(f\"generated_pages/page_{idx+1}_{page_data['title']}.png\", page_buffer.getvalue())\n",
        "\n",
        "                zip_buffer.seek(0)\n",
        "                st.session_state.export_data = zip_buffer.getvalue()\n",
        "                st.session_state.export_filename = \"handwriting_project.zip\"\n",
        "                st.session_state.export_mime = \"application/zip\"\n",
        "\n",
        "            elif export_format == \"JSON Mappings\":\n",
        "                mappings_json = json.dumps(st.session_state.character_mappings, indent=2)\n",
        "                st.session_state.export_data = mappings_json.encode()\n",
        "                st.session_state.export_filename = \"character_mappings.json\"\n",
        "                st.session_state.export_mime = \"application/json\"\n",
        "\n",
        "            elif export_format == \"Character Images Only\":\n",
        "                zip_buffer = BytesIO()\n",
        "                with zipfile.ZipFile(zip_buffer, 'w') as zip_file:\n",
        "                    for char_path in st.session_state.characters_extracted:\n",
        "                        if os.path.exists(char_path):\n",
        "                            zip_file.write(char_path, f\"characters/{os.path.basename(char_path)}\")\n",
        "\n",
        "                zip_buffer.seek(0)\n",
        "                st.session_state.export_data = zip_buffer.getvalue()\n",
        "                st.session_state.export_filename = \"character_images.zip\"\n",
        "                st.session_state.export_mime = \"application/zip\"\n",
        "\n",
        "            elif export_format == \"Generated Pages Only\":\n",
        "                if st.session_state.generated_pages:\n",
        "                    zip_buffer = BytesIO()\n",
        "                    with zipfile.ZipFile(zip_buffer, 'w') as zip_file:\n",
        "                        for idx, page_data in enumerate(st.session_state.generated_pages):\n",
        "                            page_buffer = BytesIO()\n",
        "                            page_data['image'].save(page_buffer, format='PNG')\n",
        "                            zip_file.writestr(f\"page_{idx+1}_{page_data['title']}.png\", page_buffer.getvalue())\n",
        "\n",
        "                    zip_buffer.seek(0)\n",
        "                    st.session_state.export_data = zip_buffer.getvalue()\n",
        "                    st.session_state.export_filename = \"generated_pages.zip\"\n",
        "                    st.session_state.export_mime = \"application/zip\"\n",
        "                else:\n",
        "                    st.warning(\"No generated pages to export.\")\n",
        "                    return\n",
        "\n",
        "            st.success(\"Export prepared successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Export preparation failed: {e}\")\n",
        "            return\n",
        "\n",
        "    # Show download button if export data is ready\n",
        "    if hasattr(st.session_state, 'export_data'):\n",
        "        st.download_button(\n",
        "            label=f\"📥 Download {export_format}\",\n",
        "            data=st.session_state.export_data,\n",
        "            file_name=st.session_state.export_filename,\n",
        "            mime=st.session_state.export_mime,\n",
        "            key=\"export_download\"\n",
        "        )\n",
        "\n",
        "def create_pdf_from_images(images, output_path):\n",
        "    \"\"\"Create PDF from list of images\"\"\"\n",
        "    try:\n",
        "        c = canvas.Canvas(output_path, pagesize=A4)\n",
        "        page_width, page_height = A4\n",
        "\n",
        "        for img in images:\n",
        "            if img:\n",
        "                # Convert PIL image to bytes\n",
        "                img_buffer = BytesIO()\n",
        "                img.save(img_buffer, format='PNG')\n",
        "                img_buffer.seek(0)\n",
        "\n",
        "                # Calculate scaling to fit page\n",
        "                img_width, img_height = img.size\n",
        "                scale_w = page_width / img_width\n",
        "                scale_h = page_height / img_height\n",
        "                scale = min(scale_w, scale_h) * 0.9  # 90% of page size\n",
        "\n",
        "                new_width = img_width * scale\n",
        "                new_height = img_height * scale\n",
        "\n",
        "                margin = 50\n",
        "                x = margin\n",
        "                y = page_height - new_height - margin\n",
        "\n",
        "                # Draw image\n",
        "                c.drawImage(ImageReader(img_buffer), x, y, new_width, new_height)\n",
        "                c.showPage()\n",
        "\n",
        "        c.save()\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error creating PDF: {e}\")\n",
        "        return False\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "        if st.button(\"Reset Project\", type=\"secondary\"):\n",
        "            for key in list(st.session_state.keys()):\n",
        "                if key.startswith(('processed_', 'characters_', 'character_')):\n",
        "                    del st.session_state[key]\n",
        "            st.rerun()\n",
        "\n",
        "    # Main content based on selected page\n",
        "    if page == \"Image Processing & Mapping\":\n",
        "        st.markdown('<div class=\"step-header\">📸 Image Processing & Character Mapping</div>', unsafe_allow_html=True)\n",
        "\n",
        "        # Two main sections\n",
        "        tab1, tab2 = st.tabs([\"🖼️ Image Processing\", \"🔤 Character Mapping\"])\n",
        "\n",
        "        with tab1:\n",
        "            st.markdown(\"\"\"\n",
        "            <div class=\"info-box\">\n",
        "            <strong>Instructions:</strong><br>\n",
        "            1. Upload an image containing handwritten characters<br>\n",
        "            2. The system will automatically segment and extract individual characters<br>\n",
        "            3. Characters will be processed and prepared for mapping<br>\n",
        "            4. Optional: Use OCR for automatic character recognition assistance\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "            uploaded_file = st.file_uploader(\n",
        "                \"Choose an image file\",\n",
        "                type=['png', 'jpg', 'jpeg'],\n",
        "                help=\"Upload a clear image with handwritten characters\"\n",
        "            )\n",
        "\n",
        "            if uploaded_file is not None:\n",
        "                # Check if this is a new image\n",
        "                current_hash = get_image_hash(uploaded_file)\n",
        "                is_new_image = st.session_state.processed_image_hash != current_hash\n",
        "\n",
        "                # Display uploaded image\n",
        "                col1, col2 = st.columns(2)\n",
        "                with col1:\n",
        "                    st.image(uploaded_file, caption=\"Uploaded Image\", use_container_width=True)\n",
        "\n",
        "                with col2:\n",
        "                    st.write(\"**Image Details:**\")\n",
        "                    image = Image.open(uploaded_file)\n",
        "                    st.write(f\"- **Size:** {image.size[0]} x {image.size[1]} pixels\")\n",
        "                    st.write(f\"- **Format:** {image.format}\")\n",
        "                    st.write(f\"- **Mode:** {image.mode}\")\n",
        "\n",
        "                    # OCR availability info\n",
        "                    if OCR_AVAILABLE and st.session_state.use_ocr:\n",
        "                        st.success(\"✅ OCR Enabled\")\n",
        "                    elif OCR_AVAILABLE:\n",
        "                        st.info(\"🔍 OCR Available (Disabled)\")\n",
        "                    else:\n",
        "                        st.info(\"💡 Install transformers for OCR\")\n",
        "\n",
        "                # Process button - only show if new image or not processed\n",
        "                if is_new_image or not st.session_state.processing_complete:\n",
        "                    if st.button(\"Process Image\", type=\"primary\"):\n",
        "                        with st.spinner(\"Processing image...\"):\n",
        "                            success, message = process_image_advanced(uploaded_file)\n",
        "\n",
        "                        if success:\n",
        "                            st.session_state.processed_image_hash = current_hash\n",
        "                            st.success(message)\n",
        "                            st.rerun()\n",
        "                        else:\n",
        "                            st.error(message)\n",
        "                else:\n",
        "                    st.info(\"✅ Image already processed. Results shown in Character Mapping tab.\")\n",
        "\n",
        "            # Display existing results if available\n",
        "            if st.session_state.processing_complete and st.session_state.processed_image:\n",
        "                st.write(\"### Processed Image:\")\n",
        "                st.image(st.session_state.processed_image, caption=\"Processed Image (White Background)\", use_container_width=True)\n",
        "\n",
        "                if st.session_state.characters_extracted:\n",
        "                    st.write(\"### Extracted Characters:\")\n",
        "\n",
        "                    # Options for character display\n",
        "                    col1, col2, col3 = st.columns([2, 1, 1])\n",
        "                    with col2:\n",
        "                        show_ocr = st.checkbox(\"Show OCR Predictions\",\n",
        "                                             value=False,\n",
        "                                             disabled=not (OCR_AVAILABLE and st.session_state.use_ocr))\n",
        "                    with col3:\n",
        "                        cols_per_row = st.selectbox(\"Characters per row:\", [4, 6, 8], index=1)\n",
        "\n",
        "                    display_character_grid(st.session_state.characters_extracted,\n",
        "                                         cols_per_row=cols_per_row,\n",
        "                                         show_ocr=show_ocr and st.session_state.use_ocr)\n",
        "\n",
        "        with tab2:\n",
        "            # Character mapping interface\n",
        "            if not st.session_state.characters_extracted:\n",
        "                st.warning(\"No characters extracted yet. Please process an image first in the Image Processing tab.\")\n",
        "            else:\n",
        "                st.markdown(\"### Character Verification & Mapping\")\n",
        "\n",
        "                # OCR options\n",
        "                if OCR_AVAILABLE and st.session_state.use_ocr:\n",
        "                    col1, col2 = st.columns([3, 1])\n",
        "                    with col2:\n",
        "                        if st.button(\"Run OCR on All\", type=\"secondary\"):\n",
        "                            with st.spinner(\"Running OCR...\"):\n",
        "                                for char_path in st.session_state.characters_extracted:\n",
        "                                    if char_path not in st.session_state.character_mappings or not st.session_state.character_mappings[char_path]:\n",
        "                                        ocr_result = run_ocr_on_character(char_path)\n",
        "                                        st.session_state.character_mappings[char_path] = ocr_result\n",
        "                            st.success(\"OCR completed!\")\n",
        "                            st.rerun()\n",
        "\n",
        "                # Character mapping form\n",
        "                with st.form(\"character_mapping\"):\n",
        "                    st.write(\"Map each extracted character to its actual character:\")\n",
        "\n",
        "                    # Grid layout for mapping\n",
        "                    cols_per_row = 4\n",
        "                    rows = [st.session_state.characters_extracted[i:i + cols_per_row]\n",
        "                            for i in range(0, len(st.session_state.characters_extracted), cols_per_row)]\n",
        "\n",
        "                    mappings = {}\n",
        "\n",
        "                    for row_idx, row in enumerate(rows):\n",
        "                        cols = st.columns(cols_per_row)\n",
        "\n",
        "                        for col_idx, char_path in enumerate(row):\n",
        "                            with cols[col_idx]:\n",
        "                                try:\n",
        "                                    # Display character with uniform sizing\n",
        "                                    display_img = prepare_character_for_display(char_path, target_size=(100, 100))\n",
        "                                    char_num = row_idx * cols_per_row + col_idx + 1\n",
        "                                    st.image(display_img, caption=f\"Character #{char_num}\", use_container_width=True)\n",
        "\n",
        "                                    # Get existing mapping or OCR suggestion\n",
        "                                    existing_mapping = st.session_state.character_mappings.get(char_path, \"\")\n",
        "\n",
        "                                    # OCR suggestion\n",
        "                                    ocr_suggestion = \"\"\n",
        "                                    if st.session_state.use_ocr and OCR_AVAILABLE:\n",
        "                                        ocr_suggestion = run_ocr_on_character(char_path)\n",
        "                                        if not existing_mapping and ocr_suggestion != \"Error\":\n",
        "                                            existing_mapping = ocr_suggestion\n",
        "\n",
        "                                    char_input = st.text_input(\n",
        "                                        f\"Char #{char_num}:\",\n",
        "                                        value=existing_mapping,\n",
        "                                        key=f\"char_{char_num}\",\n",
        "                                        max_chars=5,\n",
        "                                        placeholder=f\"OCR: {ocr_suggestion}\" if ocr_suggestion else \"Enter character\"\n",
        "                                    )\n",
        "                                    mappings[char_path] = char_input\n",
        "\n",
        "                                except Exception as e:\n",
        "                                    st.error(f\"Error displaying character {char_num}: {e}\")\n",
        "\n",
        "                        # Fill empty columns in the last row\n",
        "                        if len(row) < cols_per_row:\n",
        "                            for empty_col in range(len(row), cols_per_row):\n",
        "                                with cols[empty_col]:\n",
        "                                    st.empty()\n",
        "\n",
        "                    col1, col2 = st.columns(2)\n",
        "                    with col1:\n",
        "                        if st.form_submit_button(\"Save Character Mappings\", type=\"primary\"):\n",
        "                            st.session_state.character_mappings = mappings\n",
        "                            save_character_mappings()\n",
        "                            st.success(\"Character mappings saved successfully!\")\n",
        "\n",
        "                            # Display mapping summary\n",
        "                            st.write(\"### Mapping Summary:\")\n",
        "                            mapping_data = []\n",
        "                            for idx, (char_path, mapping) in enumerate(mappings.items(), 1):\n",
        "                                if mapping.strip():\n",
        "                                    mapping_data.append({\n",
        "                                        \"Character #\": idx,\n",
        "                                        \"File\": os.path.basename(char_path),\n",
        "                                        \"Mapped To\": mapping.strip()\n",
        "                                    })\n",
        "\n",
        "                            if mapping_data:\n",
        "                                df = pd.DataFrame(mapping_data)\n",
        "                                st.dataframe(df, use_container_width=True)\n",
        "\n",
        "                    with col2:\n",
        "                        if st.form_submit_button(\"Clear All Mappings\", type=\"secondary\"):\n",
        "                            st.session_state.character_mappings = {}\n",
        "                            st.warning(\"All mappings cleared!\")\n",
        "                            st.rerun()\n",
        "\n",
        "    elif page == \"Text Generation\":\n",
        "        create_text_generation_interface()\n",
        "\n",
        "    elif page == \"Analytics\":\n",
        "        create_analytics_dashboard()\n",
        "\n",
        "    elif page == \"Export\":\n",
        "        export_project()\n",
        "\n",
        "# Main App Layout\n",
        "def main():\n",
        "    # Header\n",
        "    st.markdown('<div class=\"main-header\">🖋️ HandCraft AI</div>', unsafe_allow_html=True)\n",
        "\n",
        "    # Sidebar\n",
        "    with st.sidebar:\n",
        "        st.header(\"Navigation\")\n",
        "        page = st.selectbox(\n",
        "            \"Choose a section:\",\n",
        "            [\"Image Processing & Mapping\", \"Text Generation\", \"Analytics\", \"Export\"]\n",
        "        )\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "        st.header(\"Settings\")\n",
        "\n",
        "        # OCR Toggle\n",
        "        if OCR_AVAILABLE:\n",
        "            st.session_state.use_ocr = st.checkbox(\n",
        "                \"Enable OCR Assistant\",\n",
        "                value=st.session_state.get('use_ocr', True),\n",
        "                help=\"Use AI to automatically recognize characters\"\n",
        "            )\n",
        "        else:\n",
        "            st.info(\"💡 Install `transformers torch` for OCR\")\n",
        "            st.session_state.use_ocr = False\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "        st.header(\"Project Status\")\n",
        "        if st.session_state.processing_complete:\n",
        "            st.success(\"✅ Image Processed\")\n",
        "            st.write(f\"📊 {len(st.session_state.characters_extracted)} characters extracted\")\n",
        "        else:\n",
        "            st.info(\"⏳ Awaiting Image\")\n",
        "\n",
        "        if st.session_state.character_mappings:\n",
        "            mapped_count = len([v for v in st.session_state.character_mappings.values() if v.strip()])\n",
        "            st.success(\"✅ Characters Mapped\")\n",
        "            st.write(f\"📝 {mapped_count} characters mapped\")\n",
        "        else:\n",
        "            st.info(\"⏳ Awaiting Mapping\")\n",
        "\n",
        "        # Dataset Status\n",
        "        if st.session_state.dataset_characters:\n",
        "            st.success(f\"📚 Dataset: {len(st.session_state.dataset_characters)} chars\")\n",
        "        else:\n",
        "            st.info(\"📚 No Dataset Found\")\n",
        "            st.caption(\"Create 'Dataset' folder with a.png, A.png, etc.\")\n",
        "\n",
        "        # OCR Status\n",
        "        if OCR_AVAILABLE:\n",
        "            st.success(\"🔍 OCR Available\")\n",
        "        else:\n",
        "            st.warning(\"🔍 OCR Unavailable\")\n",
        "            st.caption(\"Install: `pip install transformers torch`\")\n",
        "\n",
        "        # Speech-to-Text Status\n",
        "        if STT_AVAILABLE:\n",
        "            st.success(\"🎤 Speech-to-Text Ready\")\n",
        "        else:\n",
        "            st.info(\"🎤 STT Unavailable\")\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "        if st.button(\"Reset Project\", type=\"secondary\"):\n",
        "            for key in list(st.session_state.keys()):\n",
        "                if key.startswith(('processed_', 'characters_', 'character_')):\n",
        "                    del st.session_state[key]\n",
        "            st.rerun()\n",
        "\n",
        "    # Main content based on selected page - THIS IS THE KEY FIX\n",
        "    if page == \"Image Processing & Mapping\":\n",
        "        st.markdown('<div class=\"step-header\">📸 Image Processing & Character Mapping</div>', unsafe_allow_html=True)\n",
        "\n",
        "        # Two main sections\n",
        "        tab1, tab2 = st.tabs([\"🖼️ Image Processing\", \"🔤 Character Mapping\"])\n",
        "\n",
        "        with tab1:\n",
        "            st.markdown(\"\"\"\n",
        "            <div class=\"info-box\">\n",
        "            <strong>Instructions:</strong><br>\n",
        "            1. Upload an image containing handwritten characters<br>\n",
        "            2. The system will automatically segment and extract individual characters<br>\n",
        "            3. Characters will be processed and prepared for mapping<br>\n",
        "            4. Optional: Use OCR for automatic character recognition assistance\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "            uploaded_file = st.file_uploader(\n",
        "                \"Choose an image file\",\n",
        "                type=['png', 'jpg', 'jpeg'],\n",
        "                help=\"Upload a clear image with handwritten characters\"\n",
        "            )\n",
        "\n",
        "            if uploaded_file is not None:\n",
        "                # Check if this is a new image\n",
        "                current_hash = get_image_hash(uploaded_file)\n",
        "                is_new_image = st.session_state.processed_image_hash != current_hash\n",
        "\n",
        "                # Display uploaded image\n",
        "                col1, col2 = st.columns(2)\n",
        "                with col1:\n",
        "                    st.image(uploaded_file, caption=\"Uploaded Image\", use_container_width=True)\n",
        "\n",
        "                with col2:\n",
        "                    st.write(\"**Image Details:**\")\n",
        "                    image = Image.open(uploaded_file)\n",
        "                    st.write(f\"- **Size:** {image.size[0]} x {image.size[1]} pixels\")\n",
        "                    st.write(f\"- **Format:** {image.format}\")\n",
        "                    st.write(f\"- **Mode:** {image.mode}\")\n",
        "\n",
        "                    # OCR availability info\n",
        "                    if OCR_AVAILABLE and st.session_state.use_ocr:\n",
        "                        st.success(\"✅ OCR Enabled\")\n",
        "                    elif OCR_AVAILABLE:\n",
        "                        st.info(\"🔍 OCR Available (Disabled)\")\n",
        "                    else:\n",
        "                        st.info(\"💡 Install transformers for OCR\")\n",
        "\n",
        "                # Process button - only show if new image or not processed\n",
        "                if is_new_image or not st.session_state.processing_complete:\n",
        "                    if st.button(\"Process Image\", type=\"primary\"):\n",
        "                        with st.spinner(\"Processing image...\"):\n",
        "                            success, message = process_image_advanced(uploaded_file)\n",
        "\n",
        "                        if success:\n",
        "                            st.session_state.processed_image_hash = current_hash\n",
        "                            st.success(message)\n",
        "                            st.rerun()\n",
        "                        else:\n",
        "                            st.error(message)\n",
        "                else:\n",
        "                    st.info(\"✅ Image already processed. Results shown in Character Mapping tab.\")\n",
        "\n",
        "            # Display existing results if available\n",
        "            if st.session_state.processing_complete and st.session_state.processed_image:\n",
        "                st.write(\"### Processed Image:\")\n",
        "                st.image(st.session_state.processed_image, caption=\"Processed Image (White Background)\", use_container_width=True)\n",
        "\n",
        "                if st.session_state.characters_extracted:\n",
        "                    st.write(\"### Extracted Characters:\")\n",
        "\n",
        "                    # Options for character display\n",
        "                    col1, col2, col3 = st.columns([2, 1, 1])\n",
        "                    with col2:\n",
        "                        show_ocr = st.checkbox(\"Show OCR Predictions\",\n",
        "                                             value=False,\n",
        "                                             disabled=not (OCR_AVAILABLE and st.session_state.use_ocr))\n",
        "                    with col3:\n",
        "                        cols_per_row = st.selectbox(\"Characters per row:\", [4, 6, 8], index=1)\n",
        "\n",
        "                    display_character_grid(st.session_state.characters_extracted,\n",
        "                                         cols_per_row=cols_per_row,\n",
        "                                         show_ocr=show_ocr and st.session_state.use_ocr)\n",
        "\n",
        "        with tab2:\n",
        "            # Character mapping interface\n",
        "            if not st.session_state.characters_extracted:\n",
        "                st.warning(\"No characters extracted yet. Please process an image first in the Image Processing tab.\")\n",
        "            else:\n",
        "                create_character_mapping_interface()\n",
        "\n",
        "    elif page == \"Text Generation\":\n",
        "        create_text_generation_interface()\n",
        "\n",
        "    elif page == \"Analytics\":\n",
        "        create_analytics_dashboard()\n",
        "\n",
        "    elif page == \"Export\":\n",
        "        export_project()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAUbainiuQbk"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_MNb97_lCVu",
        "outputId": "d68c5676-839e-4c6e-c471-d0ca48d46553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "2025-08-01 08:50:06.799 Port 8501 is already in use\n",
            "🚀 Your app is live!\n",
            "🌐 Share this link: NgrokTunnel: \"https://814048ee5ab7.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "📱 Anyone can access your app with this link!\n"
          ]
        }
      ],
      "source": [
        "ngrok_token = \"30LsGu06oX4YgWEJd6z30DNO1kB_5C5VX4h5YGt3rFAUmRAqn\"  # Replace with your actual token\n",
        "\n",
        "# 4: Run Your App (With sharing - requires ngrok token)\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "import threading\n",
        "\n",
        "# Set your ngrok authentication token (replace ngrok_token with your actual token)\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "# Function to launch the Streamlit app using a system command\n",
        "def run_app():\n",
        "    !streamlit run app.py --server.headless true --server.port 8501\n",
        "\n",
        "# Terminate any active ngrok tunnels before starting a new one\n",
        "ngrok.kill()\n",
        "\n",
        "# Start the Streamlit app in a separate thread so the script can continue running\n",
        "app_thread = threading.Thread(target=run_app)\n",
        "app_thread.start()\n",
        "\n",
        "# Allow time for the Streamlit app to fully start before creating the tunnel\n",
        "time.sleep(10)\n",
        "\n",
        "# Create a public URL using ngrok and display it\n",
        "try:\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(\"🚀 Your app is live!\")\n",
        "    print(f\"🌐 Share this link: {public_url}\")\n",
        "    print(\"📱 Anyone can access your app with this link!\")\n",
        "except:\n",
        "    print(\"⚠️ Need ngrok token for sharing. App is running locally.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
